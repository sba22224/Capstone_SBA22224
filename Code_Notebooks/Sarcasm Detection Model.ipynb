{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b2fa87",
   "metadata": {},
   "source": [
    "# Compiling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5dc9fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import demoji\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing survey data\n",
    "df_survey = pd.read_excel('survey_results_data.xlsx')\n",
    "df_survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b97206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop rows where the response to the survey did not yield a label\n",
    "df_survey = df_survey.drop(df_survey[df_survey['class_survey'] == \"I don't know\"].index)\n",
    "df_survey.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae660e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_survey\n",
       "It is not sarcastic    359\n",
       "It is sarcastic        345\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution of data for each label\n",
    "df_survey[\"class_survey\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1a6558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Avoid bias from qc questions- drop duplicate text prompts and only keep one of each\n",
    "df_survey = df_survey.drop_duplicates(subset=['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d406aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with emojis: 63.64%\n"
     ]
    }
   ],
   "source": [
    "#Check the percentage of results that have emojis\n",
    "\n",
    "#Convert non-string values to string format\n",
    "df_survey['edited_text'] = df_survey['edited_text'].astype(str)\n",
    "\n",
    "#Create a regex pattern to match emojis\n",
    "emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\" #Emoticons\n",
    "                           \"\\U0001F300-\\U0001F5FF\"  #Symbols & pictographs\n",
    "                           \"\\U0001F680-\\U0001F6FF\"  #Transport & map symbols\n",
    "                           \"\\U0001F1E0-\\U0001F1FF\"  #Flags\n",
    "                           \"\\U00002702-\\U000027B0\"  #Dingbats\n",
    "                           \"\\U000024C2-\\U0001F251\"  #Enclosed characters\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#Use the regex pattern to check if the survey outputs have emojis\n",
    "df_survey['has_emoji'] = df_survey['edited_text'].apply(lambda x: bool(re.search(emoji_pattern, x)))\n",
    "\n",
    "#Calculate the percentage of rows that contain emojis\n",
    "percentage_with_emojis = (df_survey[\"has_emoji\"].sum() / len(df_survey)) * 100\n",
    "\n",
    "#Print the result\n",
    "print(f\"Percentage of rows with emojis: {percentage_with_emojis:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05fab622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_no</th>\n",
       "      <th>over_18</th>\n",
       "      <th>use_emojis</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>original_text</th>\n",
       "      <th>class_survey</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>i love shoegaze sm</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>men are so grimey</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>men are so grimey üò¢</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_no over_18 use_emojis  gender    age   \n",
       "0               1     Yes        Yes  Female  18-24  \\\n",
       "2               1     Yes        Yes  Female  18-24   \n",
       "3               1     Yes        Yes  Female  18-24   \n",
       "4               1     Yes        Yes  Female  18-24   \n",
       "5               1     Yes        Yes  Female  18-24   \n",
       "\n",
       "                                       original_text     class_survey   \n",
       "0  The only thing I got from college was a caffei...  It is sarcastic  \\\n",
       "2  \"My (extended) fam was discussing going on a t...  It is sarcastic   \n",
       "3                                 i love shoegaze sm  It is sarcastic   \n",
       "4                                  men are so grimey  It is sarcastic   \n",
       "5  okay but like the say so song aint that bad. I...  It is sarcastic   \n",
       "\n",
       "                                         edited_text  has_emoji  label  \n",
       "0  The only thing I got from college was a caffei...       True      1  \n",
       "2  \"My (extended) fam was discussing going on a t...       True      1  \n",
       "3                               i love shoegaze sm üòÅ       True      1  \n",
       "4                                men are so grimey üò¢       True      1  \n",
       "5  okay but like the say so song aint that bad. I...       True      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert labels to 0 and 1 for classification\n",
    "df_survey['label'] = df_survey['class_survey'].replace({'It is sarcastic': 1, 'It is not sarcastic': 0})\n",
    "df_survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5fbf404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_no</th>\n",
       "      <th>over_18</th>\n",
       "      <th>use_emojis</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>original_text</th>\n",
       "      <th>class_survey</th>\n",
       "      <th>tweet</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>i love shoegaze sm</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>men are so grimey</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>men are so grimey üò¢</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>It is sarcastic</td>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_no over_18 use_emojis  gender    age   \n",
       "0               1     Yes        Yes  Female  18-24  \\\n",
       "2               1     Yes        Yes  Female  18-24   \n",
       "3               1     Yes        Yes  Female  18-24   \n",
       "4               1     Yes        Yes  Female  18-24   \n",
       "5               1     Yes        Yes  Female  18-24   \n",
       "\n",
       "                                       original_text     class_survey   \n",
       "0  The only thing I got from college was a caffei...  It is sarcastic  \\\n",
       "2  \"My (extended) fam was discussing going on a t...  It is sarcastic   \n",
       "3                                 i love shoegaze sm  It is sarcastic   \n",
       "4                                  men are so grimey  It is sarcastic   \n",
       "5  okay but like the say so song aint that bad. I...  It is sarcastic   \n",
       "\n",
       "                                               tweet  has_emoji  label  \n",
       "0  The only thing I got from college was a caffei...       True      1  \n",
       "2  \"My (extended) fam was discussing going on a t...       True      1  \n",
       "3                               i love shoegaze sm üòÅ       True      1  \n",
       "4                                men are so grimey üò¢       True      1  \n",
       "5  okay but like the say so song aint that bad. I...       True      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename the edited_text column\n",
    "df_survey = df_survey.rename(columns={'edited_text': 'tweet'})\n",
    "df_survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0badb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>men are so grimey üò¢</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  The only thing I got from college was a caffei...      1\n",
       "2  \"My (extended) fam was discussing going on a t...      1\n",
       "3                               i love shoegaze sm üòÅ      1\n",
       "4                                men are so grimey üò¢      1\n",
       "5  okay but like the say so song aint that bad. I...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop all unnecessary columns for the classification task\n",
    "classification_cols = ['tweet', 'label']\n",
    "df = df_survey[classification_cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54a62d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like when professors don‚Äôt write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not ‚Äúforced‚Äù to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn‚Äôt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  sarcastic   \n",
       "0           0  The only thing I got from college is a caffein...          1  \\\n",
       "1           1  I love it when professors draw a big question ...          1   \n",
       "2           2  Remember the hundred emails from companies whe...          1   \n",
       "3           3  Today my pop-pop told me I was not ‚Äúforced‚Äù to...          1   \n",
       "4           4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1   \n",
       "\n",
       "                                            rephrase  sarcasm  irony  satire   \n",
       "0  College is really difficult, expensive, tiring...      0.0    1.0     0.0  \\\n",
       "1  I do not like when professors don‚Äôt write out ...      1.0    0.0     0.0   \n",
       "2  I, at the bare minimum, wish companies actuall...      0.0    1.0     0.0   \n",
       "3  Today my pop-pop told me I was not \"forced\" to...      1.0    0.0     0.0   \n",
       "4  I would say Ted Cruz is an asshole and doesn‚Äôt...      1.0    0.0     0.0   \n",
       "\n",
       "   understatement  overstatement  rhetorical_question  \n",
       "0             0.0            0.0                  0.0  \n",
       "1             0.0            0.0                  0.0  \n",
       "2             0.0            0.0                  0.0  \n",
       "3             0.0            0.0                  0.0  \n",
       "4             0.0            0.0                  0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing iSarcasm dataset\n",
    "df_isarc = pd.read_csv('isarcasm2022.csv')\n",
    "df_isarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d2eb970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not ‚Äúforced‚Äù to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  The only thing I got from college is a caffein...      1\n",
       "1  I love it when professors draw a big question ...      1\n",
       "2  Remember the hundred emails from companies whe...      1\n",
       "3  Today my pop-pop told me I was not ‚Äúforced‚Äù to...      1\n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I...      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop all unnecessary columns for the classification task\n",
    "classification_cols = ['tweet', 'sarcastic']\n",
    "df_isarc = df_isarc[classification_cols]\n",
    "\n",
    "#Rename the edited_text column\n",
    "df_isarc = df_isarc.rename(columns={'sarcastic': 'label'})\n",
    "\n",
    "#Show transformation\n",
    "df_isarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1956323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2601\n",
       "1     867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution of labels\n",
    "df_isarc['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3992bade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop any tweets from the iSarcasm dataset that were used in the survey as prompts to avoid duplicated entries\n",
    "\n",
    "#Get a list of unique tweets from the survey \n",
    "unique_tweets = df_survey['original_text'].unique()\n",
    "\n",
    "#Remove rows which would give duplicates of text\n",
    "df_isarc = df_isarc[~df_isarc['tweet'].isin(unique_tweets)]\n",
    "\n",
    "#Check distribution of labels\n",
    "df_isarc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "babe05d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3995, 2),\n",
       " label\n",
       " 0    2885\n",
       " 1    1110\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the iSarcasm data to the survey data\n",
    "df = pd.concat([df, df_isarc], ignore_index=True)\n",
    "\n",
    "#Check the current quantity and distribution of data\n",
    "df.shape, df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2eb2a",
   "metadata": {},
   "source": [
    "# Checking data augmentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4708ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries \n",
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "036ddb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to perform augmentation\n",
    "def augment_text(text):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d51b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>augmented_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚ÄúOven ready, shove it in the microwave‚Äù and th...</td>\n",
       "      <td>[‚Äú Oven ready, thrust it in the microwave oven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DarkenerWoW @wochinimen @MissPurplePixie shit...</td>\n",
       "      <td>[@ DarkenerWoW @ wochinimen @ MissPurplePixie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have this reoccurring nightmare where i acci...</td>\n",
       "      <td>[i make this reoccurring incubus where i by ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Trans Visibility Day all my beautiful, s...</td>\n",
       "      <td>[Happy Trans Visibility Day all my beautiful, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Should I wear my lime green pants?\" dad its 4...</td>\n",
       "      <td>[\" Should Iodin wear my lime light green pants...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text   \n",
       "0  ‚ÄúOven ready, shove it in the microwave‚Äù and th...  \\\n",
       "1  @DarkenerWoW @wochinimen @MissPurplePixie shit...   \n",
       "2  i have this reoccurring nightmare where i acci...   \n",
       "3  Happy Trans Visibility Day all my beautiful, s...   \n",
       "4  \"Should I wear my lime green pants?\" dad its 4...   \n",
       "\n",
       "                                      augmented_text  \n",
       "0  [‚Äú Oven ready, thrust it in the microwave oven...  \n",
       "1  [@ DarkenerWoW @ wochinimen @ MissPurplePixie ...  \n",
       "2  [i make this reoccurring incubus where i by ch...  \n",
       "3  [Happy Trans Visibility Day all my beautiful, ...  \n",
       "4  [\" Should Iodin wear my lime light green pants...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly select 25 rows from the df to check the effects of augmentation\n",
    "random.seed(42)\n",
    "selected_rows = random.sample(range(len(df)), 25)\n",
    "\n",
    "#Create a variable to store results\n",
    "augmented_data = {'original_text': [], 'augmented_text': []}\n",
    "\n",
    "#Augment text sample\n",
    "for row in selected_rows:\n",
    "    original_text = df.loc[row, 'tweet']\n",
    "    augmented_text = augment_text(original_text)\n",
    "    \n",
    "    augmented_data['original_text'].append(original_text)\n",
    "    augmented_data['augmented_text'].append(augmented_text)\n",
    "\n",
    "#Convert to df\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "#Check results\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d76077ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(has_emoji\n",
       " False    14\n",
       " True     11\n",
       " Name: count, dtype: int64,\n",
       " has_emoji_orig\n",
       " False    14\n",
       " True     11\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if it kept the emojis\n",
    "\n",
    "#Convert non-string values to string format\n",
    "augmented_df['augmented_text'] = augmented_df['augmented_text'].astype(str)\n",
    "augmented_df['original_text'] = augmented_df['original_text'].astype(str)\n",
    "\n",
    "#Use the regex pattern to check if the survey outputs have emojis\n",
    "augmented_df['has_emoji'] = augmented_df['augmented_text'].apply(lambda x: bool(re.search(emoji_pattern, x)))\n",
    "augmented_df['has_emoji_orig'] = augmented_df['original_text'].apply(lambda x: bool(re.search(emoji_pattern, x)))\n",
    "\n",
    "#Check distribution with emojis\n",
    "augmented_df['has_emoji'].value_counts(), augmented_df['has_emoji_orig'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecad7a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "‚ÄúOven ready, shove it in the microwave‚Äù and this man is our PM‚úåüèº\n",
      "After:\n",
      "['‚Äú Oven ready, thrust it in the microwave oven ‚Äù and this man follow our PM ‚úå üèº']\n",
      "\n",
      "Before:\n",
      "@DarkenerWoW @wochinimen @MissPurplePixie shit healer\n",
      "After:\n",
      "['@ DarkenerWoW @ wochinimen @ MissPurplePixie shit healer']\n",
      "\n",
      "Before:\n",
      "i have this reoccurring nightmare where i accidentally forget to put on shoes before school and i have to spend the whole day barefoot\n",
      "After:\n",
      "['i make this reoccurring incubus where i by chance forget to put on shoes before school day and i have to spend the whole daytime barefoot']\n",
      "\n",
      "Before:\n",
      "Happy Trans Visibility Day all my beautiful, strong, and sexy trans peeps!! We are unapologetically ourselves üíú\n",
      "After:\n",
      "['Happy Trans Visibility Day all my beautiful, unattackable, and aphrodisiac trans peeps! ! We are unapologetically ourselves üíú']\n",
      "\n",
      "Before:\n",
      "\"Should I wear my lime green pants?\" dad its 4th of july...\n",
      "After:\n",
      "['\" Should Iodin wear my lime light green pants? \" dad its fourth of july. ..']\n",
      "\n",
      "Before:\n",
      "If only people would care as much about poverty and social equality the way they do jumping up and down for Team Remain or Team Leave.\n",
      "After:\n",
      "['If only people would care as much astir impoverishment and social par the elbow room they arrange jumping up and down for Team Remain or Team Leave.']\n",
      "\n",
      "Before:\n",
      "@ThomasCabaret84 @MicrobiomDigest @raoult_didier Surely you‚Äôre not suggesting that their own thorough investigation of their own suspected &lt;fraud&gt; (which found no evidence for said &lt;fraud&gt;) was carried out with anything but the very  highest levels of integrity? Seems completely legit to me‚Ä¶\n",
      "After:\n",
      "['@ ThomasCabaret84 @ MicrobiomDigest @ raoult_didier Surely you ‚Äô re not suggesting that their own thorough investigation of their own suspected & lt; fraud & gt; (which found no evidence for said & lt; fraud & gt;) be carried out with anything simply the very highest levels of integrity? Seems totally legit to me ‚Ä¶']\n",
      "\n",
      "Before:\n",
      "Keanu reeves is killing his role in ozark season 3üòÄ\n",
      "After:\n",
      "['Keanu reeves is kill his part in ozark season trine üòÄ']\n",
      "\n",
      "Before:\n",
      "i love kyungsoo‚Äôs pictures so much they‚Äôre all so pretty i wanna share some with him too now\n",
      "After:\n",
      "['i love kyungsoo ‚Äô s pictures thence much they ‚Äô re all so pretty i wanna portion some with him overly now']\n",
      "\n",
      "Before:\n",
      "\"your pronouns are what's in your pants\" ok. juicy/dumperüòí\n",
      "After:\n",
      "['\" your pronouns be what \\' siemens in your pants \" fine. juicy / dumper üòí']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Manually evaluate results\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[0])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[0])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[1])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[1])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[2])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[2])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[3])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[3])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[4])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[4])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[5])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[5])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[6])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[6])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[7])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[7])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[8])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[8])\n",
    "print()\n",
    "print('Before:')\n",
    "print(augmented_df['original_text'].iloc[9])\n",
    "print('After:')\n",
    "print(augmented_df['augmented_text'].iloc[9])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44b06015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 64, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 64, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 64, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 64, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 64, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 72, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 72, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 72, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 72, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 72, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 53, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 53, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 53, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 53, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 53, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 84, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 84, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 84, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 84, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 84, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>paraphrased_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My dad just said to me, ‚ÄúHey, you‚Äôre kinda ski...</td>\n",
       "      <td>[My dad just said to me, ‚ÄúHey, you‚Äôre kinda sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Post university anxiety is creeping in now lik...</td>\n",
       "      <td>[Post university anxiety is creeping in now li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My department is hiring in metaphysics and epi...</td>\n",
       "      <td>[My department is hiring in metaphysics and ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seeing my friends shine is so heartwarming ü•∫ü•∫</td>\n",
       "      <td>[Seeing my friends shine is so heartwarming ü•∫ü•∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If your website still has a google plus share ...</td>\n",
       "      <td>[If your website still has a google plus share...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text   \n",
       "0  My dad just said to me, ‚ÄúHey, you‚Äôre kinda ski...  \\\n",
       "1  Post university anxiety is creeping in now lik...   \n",
       "2  My department is hiring in metaphysics and epi...   \n",
       "3      Seeing my friends shine is so heartwarming ü•∫ü•∫   \n",
       "4  If your website still has a google plus share ...   \n",
       "\n",
       "                                    paraphrased_text  \n",
       "0  [My dad just said to me, ‚ÄúHey, you‚Äôre kinda sk...  \n",
       "1  [Post university anxiety is creeping in now li...  \n",
       "2  [My department is hiring in metaphysics and ep...  \n",
       "3  [Seeing my friends shine is so heartwarming ü•∫ü•∫...  \n",
       "4  [If your website still has a google plus share...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try use of paraphrasing to augment text\n",
    "\n",
    "#Randomly select 25 rows from the df to check the effects of augmentation\n",
    "random.seed(51)\n",
    "selected_rows = random.sample(range(len(df)), 25)\n",
    "\n",
    "#Initialise text generation pipeline with pre-trained model\n",
    "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "#Function to perform text paraphrasing\n",
    "def paraphrase_text(text, num_paraphrase=5):\n",
    "    paraphrased_texts = []\n",
    "    for _ in range(num_paraphrase):\n",
    "        paraphrased_text = generator(text, max_length=200, num_return_sequences=1, do_sample=True)[0][\"generated_text\"]\n",
    "        paraphrased_texts.append(paraphrased_text)\n",
    "    return paraphrased_texts\n",
    "\n",
    "#Create a variable to store results\n",
    "paraphrased_data = {'original_text': [], 'paraphrased_text': []}\n",
    "\n",
    "#Augment text sample\n",
    "for row in selected_rows:\n",
    "    original_text = df.loc[row, 'tweet']\n",
    "    paraphrased_text = paraphrase_text(original_text)\n",
    "    \n",
    "    paraphrased_data['original_text'].append(original_text)\n",
    "    paraphrased_data['paraphrased_text'].append(paraphrased_text)\n",
    "\n",
    "#Convert to df\n",
    "paraphrased_df = pd.DataFrame(paraphrased_data)\n",
    "\n",
    "#Check results\n",
    "paraphrased_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b53bd03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(has_emoji\n",
       " False    20\n",
       " True      5\n",
       " Name: count, dtype: int64,\n",
       " has_emoji_orig\n",
       " False    23\n",
       " True      2\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if it kept the emojis\n",
    "\n",
    "#Convert non-string values to string format\n",
    "paraphrased_df['paraphrased_text'] = paraphrased_df['paraphrased_text'].astype(str)\n",
    "paraphrased_df['original_text'] = paraphrased_df['original_text'].astype(str)\n",
    "\n",
    "#Use the regex pattern to check if the survey outputs have emojis\n",
    "paraphrased_df['has_emoji'] = paraphrased_df['paraphrased_text'].apply(lambda x: bool(re.search(emoji_pattern, x)))\n",
    "paraphrased_df['has_emoji_orig'] = paraphrased_df['original_text'].apply(lambda x: bool(re.search(emoji_pattern, x)))\n",
    "\n",
    "#Check distribution with emojis\n",
    "paraphrased_df['has_emoji'].value_counts(), paraphrased_df['has_emoji_orig'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66685864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞\n",
      "After:\n",
      "['My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞ (', 'My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞\\n', 'My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞ÔøΩ', 'My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞.', 'My dad just said to me, ‚ÄúHey, you‚Äôre kinda skinny, can you slip behind the fridge to look at something?‚Äù and it might be the nicest compliment I‚Äôve ever gotten. So guys, go tell your girl you think she‚Äôs kinda skinnyü•∞\\n']\n",
      "\n",
      "Before:\n",
      "Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨\n",
      "After:\n",
      "['Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨\\n\\nSo, let‚Äôs get started‚Ä¶\\n\\nI started using this website, on February 16, 2015, but have used it since', 'Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨ I don‚Äôt think I‚Äôve seen a single semester on the anxiety since this year. We‚Äôre all in it together and', 'Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨\\n\\nThe past few days have been filled with anxiety in the form of:\\n\\ni am really excited to start university, i can‚Äô', 'Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨\\n\\nYou will be getting in the queue for college admission. I have no idea why I‚Äôm doing this, I‚Äôve already', 'Post university anxiety is creeping in now like, 1.5 months left out of 5 years üò¨\\n\\nI had anxiety, I wasn‚Äôt as good as I should‚Äôve been at school, I didn‚Äôt get good']\n",
      "\n",
      "Before:\n",
      "My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa\n",
      "After:\n",
      "['My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa ‚Äî', 'My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa ‚Äî', 'My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa (', 'My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa pic', 'My department is hiring in metaphysics and epistemology (broadly construed). 2-2 teaching load with TA support (2-1 for the first 2 years), MA program, in a city of 260k people with several locations of just about every fast food restaurant. Not bad! https://t.co/bCd3pNEMCa\\n']\n",
      "\n",
      "Before:\n",
      "Seeing my friends shine is so heartwarming ü•∫ü•∫\n",
      "After:\n",
      "['Seeing my friends shine is so heartwarming ü•∫ü•∫\\n\"I have not seen you shine so far, till I behold you!\" -\\n- Sir Ranga Chandra, Raj Ghat, Varanasi, India\\n\\nSunday', 'Seeing my friends shine is so heartwarming ü•∫ü•∫ü•∫\\n\\nüíî @iamsangram.com üéÑüç¶\\U0001f2a3‚ô°', \"Seeing my friends shine is so heartwarming ü•∫ü•∫ü•∫.\\n\\nThe only problem is i've been busy!\\n\\n(And yes, you should see what my bed is made out of:)\\n\\nI\", 'Seeing my friends shine is so heartwarming ü•∫ü•∫ü•∫\\n\\nMy mom loves my friends very much and she keeps encouraging me to get out more and do more. I usually think being more successful in life is easy but', 'Seeing my friends shine is so heartwarming ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫']\n",
      "\n",
      "Before:\n",
      "If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨\n",
      "After:\n",
      "['If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨\\nI do also have a blog but unfortunately the pictures that I post don‚Äôt do', 'If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨\\n\\nSo it seems like your blog still has a share button? I don‚Äôt', 'If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨\\n\\n partitions: 2\\n\\nPosts:\\n\\n2\\n\\nHi\\n\\nIt is', 'If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨ However, I will tell you that if you are not using it, you should be, unless', 'If your website still has a google plus share button, forgive me if Im not tripping over myself to take your information as credible or current.üò¨\\n\\nI use to be quite the fan of D&D, but I find it increasingly']\n",
      "\n",
      "Before:\n",
      "2 x sand eating toddlers for sale. No previous owners. DM for details x\n",
      "After:\n",
      "['2 x sand eating toddlers for sale. No previous owners. DM for details x', '2 x sand eating toddlers for sale. No previous owners. DM for details x\\n\\nFinance\\n\\nWe are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking', \"2 x sand eating toddlers for sale. No previous owners. DM for details x. Photos by Michael Anderson for PIXEL CITY, FLORIDA\\n\\nWe've included tons of great photos from across the world, so click on the photos to\", '2 x sand eating toddlers for sale. No previous owners. DM for details x. For more info please email: kim@pumpys.com\\n\\nHi everyone. I have a little kitty from a previous owner, and we are', '2 x sand eating toddlers for sale. No previous owners. DM for details x - [read more]\\n\\n2 x 4ft. girth sand eating toddler for sale. No previous owners. DM for details x - [read more]\\n']\n",
      "\n",
      "Before:\n",
      "@FrankHassleYT Why is his face blurred\n",
      "After:\n",
      "['@FrankHassleYT Why is his face blurred?\\n\\nhttps://twitter.com/Frank_HassleYT/status/803540233900251561\\n\\nFrom the article:\\n\\n‚Äú', \"@FrankHassleYT Why is his face blurred? Does it even matter?\\n\\nA:\\n\\nI'm not sure, but from that pic the man is wearing an Argyll Shire cap - this should put your problem\", \"@FrankHassleYT Why is his face blurred?)\\n\\nI like the idea of a full screen version, but it's not really a big draw either way. A 3:2 aspect ratio would probably work better.\\n\\nI\", '@FrankHassleYT Why is his face blurred? Is it a problem on his end? Or on his ISP? Not really sure. https://bugs.launchpad.net/ubuntu/+source/openoffice.org/+bug/', '@FrankHassleYT Why is his face blurred?\\n\\n[2]']\n",
      "\n",
      "Before:\n",
      "Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLt\n",
      "After:\n",
      "['Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLtY nEThTsR.\\n\\nI‚Äôm not looking to impress anyone or make the', 'Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLt.iS aDtT iCeA.iN mIiS aM a', 'Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLtIcE! I live in a world where people say ‚Äúf*ck‚Äù and get', 'Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLtR, anYaN aN aN aN aDuLtR. *Gigantic', 'Being 20 years old and going to concerts with my mom means I can scream all the no-no lyrics bc iM aN aDuLtY\\nAnd iM also going to be on the radio and be heard by everyone!\\nI had']\n",
      "\n",
      "Before:\n",
      "@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread.\n",
      "After:\n",
      "['@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread. Available in an assortment of flavors, each bottle is infused with the flavors of this signature cheese‚Äîbrie, blue cheese, and truffle.', '@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread. #hobobob, the ultimate all-around burger! #taste #bistro_mike\\n\\nThis is the special', '@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread. In a new release you dont have to wait long for this one as theyve introduced one of the nicest light bites yet for the summer!', '@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread. You can find it for $4. This is a vegan cheese with a crunchy texture that has a hint of sweetness. The texture is similar', '@ArenaSwansea @creedfs Specialty starter or light bite featuring laverbread.\\n\\n#10: Bluebird @cafe mocha mocha @Cafe mocha mocha.\\n\\n#']\n",
      "\n",
      "Before:\n",
      "Every time I see an establishment with paper straws I turn a little bit Republican\n",
      "After:\n",
      "['Every time I see an establishment with paper straws I turn a little bit Republican and I hate the idea of a paper straw.\\n\\nIn politics my conservative leaning is manifested in not having my lawn trimmed on the Fourth of July. And I hate', 'Every time I see an establishment with paper straws I turn a little bit Republican.\\n\\nNow that I‚Äôve gotten this out of my system, it is time I put some words to it. This is exactly what it sounds like:', 'Every time I see an establishment with paper straws I turn a little bit Republican.\" - Ted Cruz, US Senator\\n\\n\"Paper Straws\" (1953)\\n\\n\"Paper Stoppers!\" (1953)\\n\\n\"Paper Straw', 'Every time I see an establishment with paper straws I turn a little bit Republican. So that‚Äôs where I‚Äôm going to start my day. I‚Äôm going to be eating, talking, and writing about the GOP ‚Ä¶', 'Every time I see an establishment with paper straws I turn a little bit Republican.\\n\\nYesterday I decided to give the paper straws a try. They‚Äôre not bad, but the only place where they‚Äôd work is at']\n"
     ]
    }
   ],
   "source": [
    "#Manually evaluate results\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[0])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[0])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[1])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[1])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[2])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[2])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[3])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[3])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[4])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[4])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[5])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[5])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[6])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[6])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[7])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[7])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[8])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[8])\n",
    "print()\n",
    "print('Before:')\n",
    "print(paraphrased_df['original_text'].iloc[9])\n",
    "print('After:')\n",
    "print(paraphrased_df['paraphrased_text'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e7904",
   "metadata": {},
   "source": [
    "# Collect datasets for compilation with current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0474b329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcasm_ref</th>\n",
       "      <th>human_aggregated</th>\n",
       "      <th>#humans_sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would date any of these men ü•∫</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sarcasm_ref   \n",
       "0  Size on the the Toulouse team, That pack is mo...            0  \\\n",
       "1                                           Pinball!            0   \n",
       "2  So the Scottish Government want people to get ...            1   \n",
       "3  villainous pro tip : change the device name on...            0   \n",
       "4                    I would date any of these men ü•∫            0   \n",
       "\n",
       "   human_aggregated  #humans_sarcasm  \n",
       "0                 0                1  \n",
       "1                 0                0  \n",
       "2                 1                4  \n",
       "3                 0                0  \n",
       "4                 0                1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing SarcEval dataset\n",
    "df_eval = pd.read_csv('english_task_a.csv')\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed5e7fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Size on the the Toulouse team, That pack is mo...      0\n",
       " 1                                           Pinball!      0\n",
       " 2  So the Scottish Government want people to get ...      1\n",
       " 3  villainous pro tip : change the device name on...      0\n",
       " 4                    I would date any of these men ü•∫      0,\n",
       " (1400, 2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Format for the evaluation\n",
    "\n",
    "#Rename the label column\n",
    "df_eval = df_eval.rename(columns={'sarcasm_ref': 'label'})\n",
    "\n",
    "#Drop all unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_eval = df_eval[classification_cols]\n",
    "\n",
    "#Check df format and size\n",
    "df_eval.head(), df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "426817fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1  #sarcasm for #people who don't understand #diy...  figurative\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3  @wilw Why do I get the feeling you like games?...  figurative\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...  figurative"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing #sarcasm and #irony dataset\n",
    "df_add = pd.read_csv('train.csv')\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75c779b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Be aware  dirty step to get money  #staylight ...      1\n",
       " 1  #sarcasm for #people who don't understand #diy...      1\n",
       " 2  @IminworkJeremy @medsingle #DailyMail readers ...      1\n",
       " 3  @wilw Why do I get the feeling you like games?...      1\n",
       " 4  -@TeacherArthurG @rweingarten You probably jus...      1,\n",
       " (81408, 2))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Format for the evaluation\n",
    "\n",
    "#Rename the label column\n",
    "df_add = df_add.rename(columns={'tweets': 'text'})\n",
    "\n",
    "#All tweets are sarcastic- relabel them all as 1\n",
    "df_add['label'] = 1\n",
    "\n",
    "#Drop all unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add = df_add[classification_cols]\n",
    "\n",
    "#Check df format and size\n",
    "df_add.head(), df_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c62bfcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for #people who don't understand #diy #artatt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Be aware  dirty step to get money  #staylight ...      1\n",
       "1   for #people who don't understand #diy #artatt...      1\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...      1\n",
       "3    @wilw Why do I get the feeling you like games?       1\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...      1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove specific hashtags from the text\n",
    "df_add['text'] = df_add['text'].str.replace(r'#sarcasm\\b', '', regex=True)\n",
    "df_add['text'] = df_add['text'].str.replace(r'#irony\\b', '', regex=True)\n",
    "df_add['text'] = df_add['text'].str.replace(r'#sarcastic\\b', '', regex=True)\n",
    "\n",
    "#Check transformation\n",
    "df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c537c28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one ever predicted this was going to happen...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Stooshie its as closely related as Andrews or...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I find it ironic when Vegans say they love foo...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quick rt that throwing money vine I've not see...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yep, keep adding me to your #devops lists.... ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  no one ever predicted this was going to happen...  figurative\n",
       "1  @Stooshie its as closely related as Andrews or...  figurative\n",
       "2  I find it ironic when Vegans say they love foo...  figurative\n",
       "3  Quick rt that throwing money vine I've not see...  figurative\n",
       "4  yep, keep adding me to your #devops lists.... ...  figurative"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat using the test data\n",
    "\n",
    "#Importing #sarcasm and #irony dataset\n",
    "df_add1 = pd.read_csv('test.csv')\n",
    "df_add1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14d76161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  no one ever predicted this was going to happen...      1\n",
       " 1  @Stooshie its as closely related as Andrews or...      1\n",
       " 2  I find it ironic when Vegans say they love foo...      1\n",
       " 3  Quick rt that throwing money vine I've not see...      1\n",
       " 4  yep, keep adding me to your #devops lists.... ...      1,\n",
       " (8128, 2))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Format for the evaluation\n",
    "\n",
    "#Rename the label column\n",
    "df_add1 = df_add1.rename(columns={'tweets': 'text'})\n",
    "\n",
    "#All tweets are sarcastic- relabel them all as 1\n",
    "df_add1['label'] = 1\n",
    "\n",
    "#Drop all unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add1 = df_add1[classification_cols]\n",
    "\n",
    "#Remove specific hashtags from the text\n",
    "df_add1['text'] = df_add1['text'].str.replace(r'#sarcasm\\b', '', regex=True)\n",
    "df_add1['text'] = df_add1['text'].str.replace(r'#irony\\b', '', regex=True)\n",
    "df_add1['text'] = df_add1['text'].str.replace(r'#sarcastic\\b', '', regex=True)\n",
    "\n",
    "#Check df format and size\n",
    "df_add1.head(), df_add1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "347df7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Be aware  dirty step to get money  #staylight ...      1\n",
       " 1   for #people who don't understand #diy #artatt...      1\n",
       " 2  @IminworkJeremy @medsingle #DailyMail readers ...      1\n",
       " 3    @wilw Why do I get the feeling you like games?       1\n",
       " 4  -@TeacherArthurG @rweingarten You probably jus...      1,\n",
       " (89536, 2))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the dfs using the same collection strategy\n",
    "df_add = pd.concat([df_add, df_add1], ignore_index=True)\n",
    "\n",
    "#Check df size and form\n",
    "df_add.head(), df_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29d9fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    89536\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution of labels\n",
    "df_add['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a012f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>@0430yes i hope youre lurking rn. i want to li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>05 really taught me a valuable lesson I'm neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>@098BERRY Never had a voice to protest, so you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>@0hMySt4rs Rest in peace &amp; love to you and you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>100 days until Christmas! üå≤ #too soon #not rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1  Column2                                            Column3\n",
       "0  TrainSen        0  @0430yes i hope youre lurking rn. i want to li...\n",
       "1  TrainSen        0  05 really taught me a valuable lesson I'm neve...\n",
       "2  TrainSen        0  @098BERRY Never had a voice to protest, so you...\n",
       "3  TrainSen        0  @0hMySt4rs Rest in peace & love to you and you...\n",
       "4  TrainSen        0  100 days until Christmas! üå≤ #too soon #not rea..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specify the file path to read in next dataset- dataset collected and annotated using weak labels -> #not and offensive vocab\n",
    "file_path = 'Train_v1.txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add1 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Column1', 'Column2', 'Column3'])\n",
    "\n",
    "#Check the df\n",
    "df_add1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ff22f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  @0430yes i hope youre lurking rn. i want to li...      0\n",
       " 1  05 really taught me a valuable lesson I'm neve...      0\n",
       " 2  @098BERRY Never had a voice to protest, so you...      0\n",
       " 3  @0hMySt4rs Rest in peace & love to you and you...      0\n",
       " 4  100 days until Christmas! üå≤ #too soon #not rea...      0,\n",
       " (39780, 2))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename cols based on the contents\n",
    "df_add1 = df_add1.rename(columns={'Column2': 'label'})\n",
    "df_add1 = df_add1.rename(columns={'Column3': 'text'})\n",
    "\n",
    "#Drop unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add1 = df_add1[classification_cols]\n",
    "\n",
    "#Check df form and size\n",
    "df_add1.head(), df_add1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d15a07ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  I loovee when people text back ... üòí #sarcasti...      1\n",
       " 1  Don't you love it when your parents are Pissed...      1\n",
       " 2      So many useless classes , great to be student      1\n",
       " 3  Oh how I love getting home from work at 3am an...      1\n",
       " 4          I just love having grungy ass hair üòë #not      1,\n",
       " (1975, 2))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset collected and annotated using weak labels -> #not and offensive vocab\n",
    "\n",
    "#Specify the file path to read in next dataset \n",
    "file_path = 'Test_v1.txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add2 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Column1', 'label', 'text'])\n",
    "\n",
    "#Drop unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add2 = df_add2[classification_cols]\n",
    "\n",
    "#Check the df\n",
    "df_add2.head(), df_add2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01259a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  @0430yes i hope youre lurking rn. i want to li...      0\n",
       " 1  05 really taught me a valuable lesson I'm neve...      0\n",
       " 2  @098BERRY Never had a voice to protest, so you...      0\n",
       " 3  @0hMySt4rs Rest in peace & love to you and you...      0\n",
       " 4  100 days until Christmas! üå≤ #too soon #not rea...      0,\n",
       " (41755, 2))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the dfs using the same collection strategy\n",
    "df_add1 = pd.concat([df_add1, df_add2], ignore_index=True)\n",
    "\n",
    "#Check df size and form\n",
    "df_add1.head(), df_add1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5149e441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    22267\n",
       "1    19488\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution of labels\n",
    "df_add1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2e2bd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Nih min buat fans arsenal @my_supersoccer :D h...      1\n",
       " 1  Give a person power that will be a true test o...      1\n",
       " 2  @LordWilsonVILLA At 21 he looks to have a lot ...      1\n",
       " 3  I'm about to fall asleep and I still have to b...      1\n",
       " 4  I love hearing the shots from the shooting ran...      1,\n",
       " (1368, 2))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset by Riloff\n",
    "\n",
    "#Specify the file path to read in next dataset\n",
    "file_path = 'train.txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add2 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['text'])\n",
    "\n",
    "#All tweets are sarcastic- relabel them all as 1\n",
    "df_add2['label'] = 1\n",
    "\n",
    "#Check the df\n",
    "df_add2.head(), df_add2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e016e182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Absolutely love when water is spilt on my phon...      1\n",
       " 1  I was hoping just a LITTLE more shit could hit...      1\n",
       " 2  @pdomo Don't forget that Nick Foles is also th...      1\n",
       " 3  I constantly see tweets about Arsenal on twitt...      1\n",
       " 4  Can feel the feet pulsating...slow one...becau...      1,\n",
       " (588, 2))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for the train data\n",
    "\n",
    "#Specify the file path to read in next dataset\n",
    "file_path = 'test.txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add3 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['text'])\n",
    "\n",
    "#All tweets are sarcastic- relabel them all as 1\n",
    "df_add3['label'] = 1\n",
    "\n",
    "#Check the df\n",
    "df_add3.head(), df_add3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "897eae67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Nih min buat fans arsenal @my_supersoccer :D h...      1\n",
       " 1  Give a person power that will be a true test o...      1\n",
       " 2  @LordWilsonVILLA At 21 he looks to have a lot ...      1\n",
       " 3  I'm about to fall asleep and I still have to b...      1\n",
       " 4  I love hearing the shots from the shooting ran...      1,\n",
       " (1956, 2))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the dfs using the same collection strategy\n",
    "df_add2 = pd.concat([df_add2, df_add3], ignore_index=True)\n",
    "\n",
    "#Check df size and form\n",
    "df_add2.head(), df_add2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8f92cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1956\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution of labels\n",
    "df_add2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b9478eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>1</td>\n",
       "      <td>It feels like just a few days ago it was the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>1</td>\n",
       "      <td>I love my mom . No matter what we go through ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>1</td>\n",
       "      <td>Bump that music ... #imtryingtosleep #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican and black jokes are pretty much the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrainSen</td>\n",
       "      <td>0</td>\n",
       "      <td>How to find work you love :</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1  Column2                                            Column3\n",
       "0  TrainSen        1  It feels like just a few days ago it was the l...\n",
       "1  TrainSen        1  I love my mom . No matter what we go through ,...\n",
       "2  TrainSen        1      Bump that music ... #imtryingtosleep #sarcasm\n",
       "3  TrainSen        0  Mexican and black jokes are pretty much the sa...\n",
       "4  TrainSen        0                        How to find work you love :"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset by Ghosh- uses weak labelling with #sarcasm\n",
    "\n",
    "#Specify the file path to read in next dataset\n",
    "file_path = 'train (1).txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add3 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Column1', 'Column2', 'Column3'])\n",
    "\n",
    "#Check the df\n",
    "df_add3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f6532f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  It feels like just a few days ago it was the l...      1\n",
       " 1  I love my mom . No matter what we go through ,...      1\n",
       " 2              Bump that music ... #imtryingtosleep       1\n",
       " 3  Mexican and black jokes are pretty much the sa...      0\n",
       " 4                        How to find work you love :      0,\n",
       " (51189, 2))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename cols based on the contents\n",
    "df_add3 = df_add3.rename(columns={'Column2': 'label'})\n",
    "df_add3 = df_add3.rename(columns={'Column3': 'text'})\n",
    "\n",
    "#Drop unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add3 = df_add3[classification_cols]\n",
    "\n",
    "#Remove weak label\n",
    "df_add3['text'] = df_add3['text'].str.replace(r'#sarcasm\\b', '', regex=True)\n",
    "\n",
    "#Check df form and size\n",
    "df_add3.head(), df_add3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e26b06ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  It feels like just a few days ago it was the l...      1\n",
       " 1  I love my mom . No matter what we go through ,...      1\n",
       " 2              Bump that music ... #imtryingtosleep       1\n",
       " 3  Mexican and black jokes are pretty much the sa...      0\n",
       " 4                        How to find work you love :      0,\n",
       " (54877, 2))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for test data\n",
    "\n",
    "#Specify the file path to read in next dataset\n",
    "file_path = 'test (1).txt' \n",
    "\n",
    "#Read the data into a df\n",
    "df_add4 = pd.read_csv(file_path, delimiter='\\t', header=None, names=['Column1', 'label', 'text'])\n",
    "\n",
    "#Drop unnecessary columns\n",
    "classification_cols = ['text', 'label']\n",
    "df_add4 = df_add4[classification_cols]\n",
    "\n",
    "#Remove weak label\n",
    "df_add4['text'] = df_add4['text'].str.replace(r'#sarcasm\\b', '', regex=True)\n",
    "\n",
    "#Combine the dfs using the same collection strategy\n",
    "df_add3 = pd.concat([df_add3, df_add4], ignore_index=True)\n",
    "\n",
    "#Check df form and size\n",
    "df_add3.head(), df_add3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4161bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "#Compile data with weak labels consisting of offensive vocab and key hashtags\n",
    "\n",
    "#Open the csv file to adjust format and define a new one for the corrected formatting\n",
    "with open('dataset_csv.csv', 'r', encoding='utf-8') as input_file, \\\n",
    "     open('output.csv', 'w', encoding='utf-8', newline='') as output_file:\n",
    "\n",
    "    csv_reader = csv.reader(input_file)\n",
    "    csv_writer = csv.writer(output_file)\n",
    "\n",
    "    #Where row has tweet split into several cells, concatenate into a single cell\n",
    "    for row in csv_reader:\n",
    "        if len(row) > 1:  \n",
    "            full_tweet = ' '.join(row[1:])\n",
    "            csv_writer.writerow([row[0], full_tweet])\n",
    "        else:\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "print(\"CSV file preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9acca458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love working midnights tweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate when I buy a bag of air and there's chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my grandad always sounds so ill when i speak t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I realize I'm annoying to everyone, so I won't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love when I find these dudes on vine!! #Foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets label\n",
       "0                    I love working midnights tweet      1\n",
       "1  I hate when I buy a bag of air and there's chi...     1\n",
       "2  my grandad always sounds so ill when i speak t...     0\n",
       "3  I realize I'm annoying to everyone, so I won't...     0\n",
       "4  I love when I find these dudes on vine!! #Foll...     1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import corrected data and check the format\n",
    "df_add4 = pd.read_csv('output.csv', delimiter='\\t')\n",
    "df_add4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebaf2154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text label\n",
       " 0                    I love working midnights tweet      1\n",
       " 1  I hate when I buy a bag of air and there's chi...     1\n",
       " 2  my grandad always sounds so ill when i speak t...     0\n",
       " 3  I realize I'm annoying to everyone, so I won't...     0\n",
       " 4  I love when I find these dudes on vine!! #Foll...     1,\n",
       " (1994, 2))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adjust column naming for continuity \n",
    "df_add4 = df_add4.rename(columns={'tweets': 'text'})\n",
    "\n",
    "#Check data format and size\n",
    "df_add4.head(), df_add4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed40533",
   "metadata": {},
   "source": [
    "# Filter for sarcastic tweets with similar important features to the sarcastic features identified before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2aa6723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89536, 2), (41755, 2), (1956, 2), (54877, 2), (1994, 2))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape of the data\n",
    "df_add.shape, df_add1.shape, df_add2.shape, df_add3.shape, df_add4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d745892",
   "metadata": {},
   "source": [
    "# Prepare baseline data for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4c495a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>men are so grimey üò¢</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  The only thing I got from college was a caffei...      1\n",
       "1  \"My (extended) fam was discussing going on a t...      1\n",
       "2                               i love shoegaze sm üòÅ      1\n",
       "3                                men are so grimey üò¢      1\n",
       "4  okay but like the say so song aint that bad. I...      1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check structure of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7eae151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    2885\n",
      "1    1110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check distribution of labels\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2276af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet    1\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for null values \n",
    "print(df.isnull().sum())\n",
    "\n",
    "#Drop rows with null values\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7cc82cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out sarcastic tweets only\n",
    "df_sarc = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c5f0865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3074321459.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sarc['sentiment_result'] = df_sarc['tweet'].apply(calculate_probabilities)\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3074321459.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sarc['sentiment_label'] = df_sarc['sentiment_result'].apply(determine_sentiment)\n"
     ]
    }
   ],
   "source": [
    "#Get sentiment data for each row\n",
    "df_sarc['sentiment_result'] = df_sarc['tweet'].apply(calculate_probabilities)\n",
    "\n",
    "#Use the data to determine positive/negative rows\n",
    "df_sarc['sentiment_label'] = df_sarc['sentiment_result'].apply(determine_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4b7d01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create positive and negative subsets \n",
    "df_pos = df_sarc[(df_sarc['sentiment_label'] == 'positive')]\n",
    "df_neg = df_sarc[(df_sarc['sentiment_label'] == 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9811a7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_result</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1715908795595169, 0.5636150240898132, 0.264...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.21971991658210754, 0.37854474782943726, 0.4...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0015152746345847845, 0.00982738845050335, 0...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02680261805653572, 0.2406616508960724, 0.73...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tinnitus is my favourite thing ever I love it....</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0021385003346949816, 0.0065424032509326935,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label   \n",
       "0  The only thing I got from college was a caffei...      1  \\\n",
       "1  \"My (extended) fam was discussing going on a t...      1   \n",
       "2                               i love shoegaze sm üòÅ      1   \n",
       "4  okay but like the say so song aint that bad. I...      1   \n",
       "5  Tinnitus is my favourite thing ever I love it....      1   \n",
       "\n",
       "                                    sentiment_result sentiment_label  \n",
       "0  [0.1715908795595169, 0.5636150240898132, 0.264...        positive  \n",
       "1  [0.21971991658210754, 0.37854474782943726, 0.4...        positive  \n",
       "2  [0.0015152746345847845, 0.00982738845050335, 0...        positive  \n",
       "4  [0.02680261805653572, 0.2406616508960724, 0.73...        positive  \n",
       "5  [0.0021385003346949816, 0.0065424032509326935,...        positive  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check df\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169522a",
   "metadata": {},
   "source": [
    "# Break down each new dataset by polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "67fd8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    89536\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    22267\n",
      "1    19488\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    1956\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    29016\n",
      "1    25861\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0     987\n",
      "1     958\n",
      "0\"      9\n",
      "1\"      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Sarcastic tweets only\n",
    "print(df_add['label'].value_counts())\n",
    "print(df_add1['label'].value_counts())\n",
    "print(df_add2['label'].value_counts())\n",
    "print(df_add3['label'].value_counts())\n",
    "print(df_add4['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cafaf551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    987\n",
      "1    958\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Remove unusually formatted labels from the last df\n",
    "df_add4 = df_add4[df_add4['label'] != '0\"']\n",
    "df_add4 = df_add4[df_add4['label'] != '1\"']\n",
    "\n",
    "#Check data \n",
    "print(df_add4['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bca8ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     object\n",
      "label     int64\n",
      "dtype: object\n",
      "text     object\n",
      "label     int64\n",
      "dtype: object\n",
      "text     object\n",
      "label     int64\n",
      "dtype: object\n",
      "text     object\n",
      "label     int64\n",
      "dtype: object\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check datatypes for each set\n",
    "print(df_add.dtypes)\n",
    "print(df_add1.dtypes)\n",
    "print(df_add2.dtypes)\n",
    "print(df_add3.dtypes)\n",
    "print(df_add4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a12d80af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     2\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text      0\n",
      "label    38\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for null values \n",
    "print(df_add.isnull().sum())\n",
    "print(df_add1.isnull().sum())\n",
    "print(df_add2.isnull().sum())\n",
    "print(df_add3.isnull().sum())\n",
    "print(df_add4.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "177a57cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with null values\n",
    "df_add.dropna(axis=0, inplace=True)\n",
    "df_add4.dropna(axis=0, inplace=True)\n",
    "\n",
    "#Confirm no null values\n",
    "print(df_add.isnull().sum())\n",
    "print(df_add1.isnull().sum())\n",
    "print(df_add2.isnull().sum())\n",
    "print(df_add3.isnull().sum())\n",
    "print(df_add4.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2a2e5039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     object\n",
      "label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Convert object to integer\n",
    "df_add4['label'] = df_add4['label'].astype('int64')\n",
    "\n",
    "#Check\n",
    "print(df_add4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2fa3d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    89534\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    19488\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    1956\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    25861\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    958\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Pull out sarcastic tweets only where needed\n",
    "df_add1_sarc = df_add1[df_add1['label'] == 1]\n",
    "df_add3_sarc = df_add3[df_add3['label'] == 1]\n",
    "df_add4_sarc = df_add4[df_add4['label'] == 1]\n",
    "\n",
    "\n",
    "#Confirm nothing is missed\n",
    "print(df_add['label'].value_counts())\n",
    "print(df_add1_sarc['label'].value_counts())\n",
    "print(df_add2['label'].value_counts())\n",
    "print(df_add3_sarc['label'].value_counts())\n",
    "print(df_add4_sarc['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ce7953a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break down into positive and negative subsets to get the best outcomes\n",
    "\n",
    "#Use the same sentiment polarity model as before\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "#Create the sentiment analysis pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "#Function to calculate probabilities\n",
    "def calculate_probabilities(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0bb992f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3467284177.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add1_sarc['sentiment_result'] = df_add1_sarc['text'].apply(calculate_probabilities)\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3467284177.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add3_sarc['sentiment_result'] = df_add3_sarc['text'].apply(calculate_probabilities)\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3467284177.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add4_sarc['sentiment_result'] = df_add4_sarc['text'].apply(calculate_probabilities)\n"
     ]
    }
   ],
   "source": [
    "#Apply function to each df\n",
    "df_add['sentiment_result'] = df_add['text'].apply(calculate_probabilities)\n",
    "df_add1_sarc['sentiment_result'] = df_add1_sarc['text'].apply(calculate_probabilities)\n",
    "df_add2['sentiment_result'] = df_add2['text'].apply(calculate_probabilities)\n",
    "df_add3_sarc['sentiment_result'] = df_add3_sarc['text'].apply(calculate_probabilities)\n",
    "df_add4_sarc['sentiment_result'] = df_add4_sarc['text'].apply(calculate_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c1b51b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine sentiment label based on probabilities\n",
    "def determine_sentiment(probabilities):\n",
    "    if probabilities[2] > probabilities[0]:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51b40970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\473457995.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add1_sarc['sentiment_label'] = df_add1_sarc['sentiment_result'].apply(determine_sentiment)\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\473457995.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add3_sarc['sentiment_label'] = df_add3_sarc['sentiment_result'].apply(determine_sentiment)\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\473457995.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add4_sarc['sentiment_label'] = df_add4_sarc['sentiment_result'].apply(determine_sentiment)\n"
     ]
    }
   ],
   "source": [
    "#Apply function to each df\n",
    "df_add['sentiment_label'] = df_add['sentiment_result'].apply(determine_sentiment)\n",
    "df_add1_sarc['sentiment_label'] = df_add1_sarc['sentiment_result'].apply(determine_sentiment)\n",
    "df_add2['sentiment_label'] = df_add2['sentiment_result'].apply(determine_sentiment)\n",
    "df_add3_sarc['sentiment_label'] = df_add3_sarc['sentiment_result'].apply(determine_sentiment)\n",
    "df_add4_sarc['sentiment_label'] = df_add4_sarc['sentiment_result'].apply(determine_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8151563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate positive and negative subsets for each df\n",
    "add_pos = df_add[(df_add['sentiment_label'] == 'positive')]\n",
    "add_neg = df_add[(df_add['sentiment_label'] == 'negative')]\n",
    "\n",
    "add1_pos = df_add1_sarc[(df_add1_sarc['sentiment_label'] == 'positive')]\n",
    "add1_neg = df_add1_sarc[(df_add1_sarc['sentiment_label'] == 'negative')]\n",
    "\n",
    "add2_pos = df_add2[(df_add2['sentiment_label'] == 'positive')]\n",
    "add2_neg = df_add2[(df_add2['sentiment_label'] == 'negative')]\n",
    "\n",
    "add3_pos = df_add3_sarc[(df_add3_sarc['sentiment_label'] == 'positive')]\n",
    "add3_neg = df_add3_sarc[(df_add3_sarc['sentiment_label'] == 'negative')]\n",
    "\n",
    "add4_pos = df_add4_sarc[(df_add4_sarc['sentiment_label'] == 'positive')]\n",
    "add4_neg = df_add4_sarc[(df_add4_sarc['sentiment_label'] == 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c4338845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_result</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02273799665272236, 0.6872389316558838, 0.29...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@raaachf for the car ride when I get to listen...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.004537248983979225, 0.056863728910684586, 0...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aamir calls #BajrangiBhaijaan as Salman's best...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.015803156420588493, 0.35204407572746277, 0....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@stl7thward @LauraKHettiger I thought hot spot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.06396440416574478, 0.3994062542915344, 0.53...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I dont think any TV show could be more #Epic t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0027551085222512484, 0.015218231827020645, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label   \n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...      1  \\\n",
       "8   @raaachf for the car ride when I get to listen...      1   \n",
       "9   Aamir calls #BajrangiBhaijaan as Salman's best...      1   \n",
       "13  @stl7thward @LauraKHettiger I thought hot spot...      1   \n",
       "15  I dont think any TV show could be more #Epic t...      1   \n",
       "\n",
       "                                     sentiment_result sentiment_label  \n",
       "5   [0.02273799665272236, 0.6872389316558838, 0.29...        positive  \n",
       "8   [0.004537248983979225, 0.056863728910684586, 0...        positive  \n",
       "9   [0.015803156420588493, 0.35204407572746277, 0....        positive  \n",
       "13  [0.06396440416574478, 0.3994062542915344, 0.53...        positive  \n",
       "15  [0.0027551085222512484, 0.015218231827020645, ...        positive  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check df\n",
    "add_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c65a77",
   "metadata": {},
   "source": [
    "# Define boundaries for the sarcastic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "528c47e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>fear</th>\n",
       "      <th>surprise</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>trust</th>\n",
       "      <th>pred_neg</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>pred_neut</th>\n",
       "      <th>pred_sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üá∏üá∞</td>\n",
       "      <td>0.074503</td>\n",
       "      <td>0.084563</td>\n",
       "      <td>0.061850</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>0.063562</td>\n",
       "      <td>0.081030</td>\n",
       "      <td>0.083386</td>\n",
       "      <td>0.061693</td>\n",
       "      <td>0.154219</td>\n",
       "      <td>0.784088</td>\n",
       "      <td>0.092526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üëî</td>\n",
       "      <td>0.073802</td>\n",
       "      <td>0.074975</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>0.080874</td>\n",
       "      <td>0.072140</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.080391</td>\n",
       "      <td>0.084093</td>\n",
       "      <td>0.316915</td>\n",
       "      <td>0.099384</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>-0.217531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üåÄ</td>\n",
       "      <td>0.080796</td>\n",
       "      <td>0.087879</td>\n",
       "      <td>0.073162</td>\n",
       "      <td>0.076530</td>\n",
       "      <td>0.076504</td>\n",
       "      <td>0.071957</td>\n",
       "      <td>0.085254</td>\n",
       "      <td>0.080753</td>\n",
       "      <td>0.087130</td>\n",
       "      <td>0.409846</td>\n",
       "      <td>0.503024</td>\n",
       "      <td>0.322716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üöæ</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>0.087601</td>\n",
       "      <td>0.106218</td>\n",
       "      <td>0.100683</td>\n",
       "      <td>0.084863</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.095245</td>\n",
       "      <td>0.110732</td>\n",
       "      <td>0.145850</td>\n",
       "      <td>0.254009</td>\n",
       "      <td>0.600141</td>\n",
       "      <td>0.108159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üëπ</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>0.077376</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>0.071265</td>\n",
       "      <td>0.070346</td>\n",
       "      <td>0.052338</td>\n",
       "      <td>0.078957</td>\n",
       "      <td>0.085835</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.172381</td>\n",
       "      <td>0.642231</td>\n",
       "      <td>-0.013007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji     anger       joy   sadness      fear  surprise   disgust   \n",
       "0    üá∏üá∞  0.074503  0.084563  0.061850  0.068795  0.072184  0.063562  \\\n",
       "1     üëî  0.073802  0.074975  0.074787  0.080874  0.072140  0.048375   \n",
       "2     üåÄ  0.080796  0.087879  0.073162  0.076530  0.076504  0.071957   \n",
       "3     üöæ  0.079520  0.087601  0.106218  0.100683  0.084863  0.054167   \n",
       "4     üëπ  0.064922  0.077376  0.070871  0.071265  0.070346  0.052338   \n",
       "\n",
       "   anticipation     trust  pred_neg  pred_pos  pred_neut  pred_sent_score  \n",
       "0      0.081030  0.083386  0.061693  0.154219   0.784088         0.092526  \n",
       "1      0.080391  0.084093  0.316915  0.099384   0.583700        -0.217531  \n",
       "2      0.085254  0.080753  0.087130  0.409846   0.503024         0.322716  \n",
       "3      0.095245  0.110732  0.145850  0.254009   0.600141         0.108159  \n",
       "4      0.078957  0.085835  0.185388  0.172381   0.642231        -0.013007  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import df containing sentiment information for all emojis\n",
    "emotion_info = pd.read_csv('emoji_emotion_stats_all.csv')\n",
    "emotion_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b306a1",
   "metadata": {},
   "source": [
    "# Emoji-Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "94c6af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üò¨', 'üò¨', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÅ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üôà', 'üôà', 'üôà', 'üôà', 'üòê', 'üòê', 'üíÖ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üßê', 'üßê', 'üòú', 'üòú', 'üòú', 'üòú', 'üòú', 'üí™', 'üí™', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üëç', 'üëç', 'üëç', 'üëç', 'üëç', 'üëç', 'üëç', 'üëç', 'üòÉ', 'üòÉ', 'ü¶∏\\u200d‚ôÄÔ∏è', 'üíÅ\\u200d‚ôÄÔ∏è', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üòç', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üòä', 'üòä', 'üòä', 'üòä', 'üòå', 'üòå', 'üòå', 'üòå', 'üòå', 'üòù', 'üòù', 'üòù', 'üòù', 'üòù', 'üòù', 'üò°', 'üò°', 'üíõ', 'üíÉ', 'üíÉ', 'ü§£', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'ü•∞', 'üôÇ', 'üôÇ', 'üòò', 'üòò', 'üòò', 'üòò', 'üòò', 'üòò', 'üòÄ', 'üòÄ', 'üòÄ', 'üòÄ', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'ü§°', 'ü§°', 'üò§', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üíã', 'üíã', 'üò¢', 'üò¢', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', '‚ù§Ô∏è', 'üò≥', 'üò≥', 'üò≥', 'üò≥', '‚ò∫Ô∏è', '‚ò∫Ô∏è', '‚ò∫Ô∏è', '‚ò∫Ô∏è', 'üí™üèª', 'üôä', 'üôä', 'üòö', 'üç∑', 'üôÑ', 'ü§¨', 'üôåüèº', 'üëäüèª', 'üê¶', 'üß°', 'üòÑ', 'ü••', 'üçÜ', 'üå∂', 'üåÑ', 'ü§ó', 'ü¶∑', 'üåô', 'üç´', 'üç∞', 'üç©', 'üç©', 'üçï', 'üòá', 'ü•∫', 'üò™', 'üò™', 'üèÄ', 'üî•', 'üåõ', 'üåú', 'üê®', 'üí§', 'üò¥', 'üò¥', 'üëçüèª', 'üíì', 'üö®', 'üëçüèΩ', 'üöì', 'üöî', '‚≠ï', 'üíï', 'üíï', 'üíï', 'üíï', 'üíò', 'üëåüèª', 'üëåüèª', '‚úåüèª', '‚úåüèª', 'üò©', 'üò©', 'üôè', 'üôè', 'üëè', 'ü§î', 'ü§î', 'üëåüèº', 'üò´', 'üçπ', 'üéâ', 'üë®', 'ü•≥', 'ü•≥', 'ü•¥', 'ü•¥', 'üëå', 'üòë', 'üòë', 'üí´', 'üòï', 'üòµ', 'üçû', 'ü•ñ', 'ü•ê', 'üò†', '‚ú®', 'üé•', 'üó£', 'ü§™', 'ü§™', 'ü§†', 'üòõ', 'üéÇ', 'üé∂', 'üòé', 'üë∂üèº', 'üñïüèª']\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the sent_score for positive sarcastic tweets\n",
    "\n",
    "#Generate a list of the emojis in the verified data collection method tweets set- negative subset\n",
    "\n",
    "#Create an empty dictionary to store emoji counts\n",
    "sarc_emojis_count = {}\n",
    "\n",
    "#Iterate through each sarcastic tweet text\n",
    "for text in df_pos['tweet']:\n",
    "    emojis = demoji.findall(text)\n",
    "    \n",
    "    #Count the occurrence of each emoji in the current text\n",
    "    for emoji in emojis:\n",
    "        if emoji in sarc_emojis_count:\n",
    "            sarc_emojis_count[emoji] += 1\n",
    "        else:\n",
    "            sarc_emojis_count[emoji] = 1\n",
    "\n",
    "#Convert the dictionary into a list where emojis are repeated based on their count\n",
    "sarc_emojis_list = []\n",
    "for emoji, count in sarc_emojis_count.items():\n",
    "    sarc_emojis_list.extend([emoji] * count)\n",
    "    \n",
    "#Check the list\n",
    "print(sarc_emojis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dd49d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: üòè, Sentiment Score: 0.31183\n",
      "Emoji: üò¨, Sentiment Score: -0.42355\n",
      "Emoji: üòÅ, Sentiment Score: 0.74278\n",
      "Emoji: üòÇ, Sentiment Score: 0.13526\n",
      "Emoji: üôà, Sentiment Score: -0.05900\n",
      "Emoji: üòê, Sentiment Score: -0.38284\n",
      "Emoji: üíÖ, Sentiment Score: 0.25239\n",
      "Emoji: üôÉ, Sentiment Score: -0.13914\n",
      "Emoji: üßê, Sentiment Score: 0.12837\n",
      "Emoji: üòú, Sentiment Score: 0.52508\n",
      "Emoji: üí™, Sentiment Score: 0.44248\n",
      "Emoji: üòÖ, Sentiment Score: 0.44144\n",
      "Emoji: üëç, Sentiment Score: 0.55644\n",
      "Emoji: üòÉ, Sentiment Score: 0.39945\n",
      "Emoji: ü¶∏‚Äç‚ôÄÔ∏è, Sentiment Score: 0.08680\n",
      "Emoji: üíÅ‚Äç‚ôÄÔ∏è, Sentiment Score: 0.14597\n",
      "Emoji: üòç, Sentiment Score: 0.79574\n",
      "Emoji: üíÄ, Sentiment Score: -0.09748\n",
      "Emoji: üòä, Sentiment Score: 0.81604\n",
      "Emoji: üòå, Sentiment Score: 0.58776\n",
      "Emoji: üòù, Sentiment Score: 0.42542\n",
      "Emoji: üò°, Sentiment Score: -0.63490\n",
      "Emoji: üíõ, Sentiment Score: 0.70975\n",
      "Emoji: üíÉ, Sentiment Score: 0.47256\n",
      "Emoji: ü§£, Sentiment Score: 0.11362\n",
      "Emoji: üò≠, Sentiment Score: -0.24413\n",
      "Emoji: ü•∞, Sentiment Score: 0.89894\n",
      "Emoji: üôÇ, Sentiment Score: 0.42841\n",
      "Emoji: üòò, Sentiment Score: 0.64959\n",
      "Emoji: üòÄ, Sentiment Score: 0.51481\n",
      "Emoji: üòí, Sentiment Score: -0.48609\n",
      "Emoji: ü§°, Sentiment Score: -0.36525\n",
      "Emoji: üò§, Sentiment Score: -0.40492\n",
      "Emoji: üòâ, Sentiment Score: 0.66413\n",
      "Emoji: üíã, Sentiment Score: 0.71569\n",
      "Emoji: üò¢, Sentiment Score: -0.57921\n",
      "Emoji: ‚ù§Ô∏è, Sentiment Score: 0.87261\n",
      "Emoji: üò≥, Sentiment Score: -0.22367\n",
      "Emoji: ‚ò∫Ô∏è, Sentiment Score: 0.78760\n",
      "Emoji: üí™üèª, Sentiment Score: 0.57104\n",
      "Emoji: üôä, Sentiment Score: 0.36187\n",
      "Emoji: üòö, Sentiment Score: 0.37616\n",
      "Emoji: üç∑, Sentiment Score: 0.18835\n",
      "Emoji: üôÑ, Sentiment Score: -0.54603\n",
      "Emoji: ü§¨, Sentiment Score: -0.37938\n",
      "Emoji: üôåüèº, Sentiment Score: 0.82376\n",
      "Emoji: üëäüèª, Sentiment Score: 0.73733\n",
      "Emoji: üê¶, Sentiment Score: 0.17539\n",
      "Emoji: üòÑ, Sentiment Score: 0.40349\n",
      "Emoji: üçÜ, Sentiment Score: 0.28344\n",
      "Emoji: üå∂, Sentiment Score: 0.24197\n",
      "Emoji: üåÑ, Sentiment Score: 0.29432\n",
      "Emoji: ü§ó, Sentiment Score: 0.70781\n",
      "Emoji: üåô, Sentiment Score: 0.34332\n",
      "Emoji: üç´, Sentiment Score: 0.20357\n",
      "Emoji: üç∞, Sentiment Score: 0.31906\n",
      "Emoji: üç©, Sentiment Score: 0.20782\n",
      "Emoji: üçï, Sentiment Score: 0.24401\n",
      "Emoji: üòá, Sentiment Score: 0.40950\n",
      "Emoji: ü•∫, Sentiment Score: 0.18869\n",
      "Emoji: üò™, Sentiment Score: -0.48022\n",
      "Emoji: üèÄ, Sentiment Score: 0.18162\n",
      "Emoji: üî•, Sentiment Score: 0.40631\n",
      "Emoji: üåõ, Sentiment Score: 0.32033\n",
      "Emoji: üåú, Sentiment Score: 0.30475\n",
      "Emoji: üê®, Sentiment Score: 0.19040\n",
      "Emoji: üí§, Sentiment Score: 0.48136\n",
      "Emoji: üò¥, Sentiment Score: -0.35049\n",
      "Emoji: üëçüèª, Sentiment Score: 0.63512\n",
      "Emoji: üíì, Sentiment Score: 0.68429\n",
      "Emoji: üö®, Sentiment Score: 0.14904\n",
      "Emoji: üëçüèΩ, Sentiment Score: 0.63943\n",
      "Emoji: üöì, Sentiment Score: 0.10603\n",
      "Emoji: üöî, Sentiment Score: -0.03714\n",
      "Emoji: ‚≠ï, Sentiment Score: -0.00148\n",
      "Emoji: üíï, Sentiment Score: 0.73571\n",
      "Emoji: üíò, Sentiment Score: 0.69053\n",
      "Emoji: üëåüèª, Sentiment Score: 0.66599\n",
      "Emoji: üò©, Sentiment Score: -0.33688\n",
      "Emoji: üôè, Sentiment Score: 0.47848\n",
      "Emoji: üëè, Sentiment Score: 0.36764\n",
      "Emoji: ü§î, Sentiment Score: 0.06137\n",
      "Emoji: üëåüèº, Sentiment Score: 0.75302\n",
      "Emoji: üò´, Sentiment Score: -0.52948\n",
      "Emoji: üçπ, Sentiment Score: 0.20896\n",
      "Emoji: üéâ, Sentiment Score: 0.88550\n",
      "Emoji: üë®, Sentiment Score: 0.10679\n",
      "Emoji: ü•¥, Sentiment Score: -0.20188\n",
      "Emoji: üëå, Sentiment Score: 0.50897\n",
      "Emoji: üòë, Sentiment Score: -0.54640\n",
      "Emoji: üí´, Sentiment Score: 0.42008\n",
      "Emoji: üòï, Sentiment Score: -0.49333\n",
      "Emoji: üòµ, Sentiment Score: -0.32580\n",
      "Emoji: üçû, Sentiment Score: 0.33070\n",
      "Emoji: ü•ñ, Sentiment Score: 0.52005\n",
      "Emoji: ü•ê, Sentiment Score: 0.38603\n",
      "Emoji: üò†, Sentiment Score: -0.53355\n",
      "Emoji: ‚ú®, Sentiment Score: 0.58372\n",
      "Emoji: üé•, Sentiment Score: 0.41565\n",
      "Emoji: üó£, Sentiment Score: -0.05609\n",
      "Emoji: ü§™, Sentiment Score: 0.39910\n",
      "Emoji: ü§†, Sentiment Score: -0.00882\n",
      "Emoji: üòõ, Sentiment Score: 0.56866\n",
      "Emoji: üéÇ, Sentiment Score: 0.41546\n",
      "Emoji: üé∂, Sentiment Score: 0.23675\n",
      "Emoji: üòé, Sentiment Score: 0.46172\n",
      "Emoji: üë∂üèº, Sentiment Score: 0.09765\n"
     ]
    }
   ],
   "source": [
    "#Check the average sent_score for each emotion in the list\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "        \n",
    "#Check the dictionary\n",
    "for emoji, sentiment_score in sarc_emoji_sentiment.items():\n",
    "    print(f\"Emoji: {emoji}, Sentiment Score: {sentiment_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "09f58d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.3231850686804263\n",
      "Standard Deviation of Sentiment Scores: 0.4116289547140633\n"
     ]
    }
   ],
   "source": [
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1848e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5000728407477003 1.1464429781085528\n"
     ]
    }
   ],
   "source": [
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "sent_score_pos_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "sent_score_pos_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "print(sent_score_pos_ll, sent_score_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5ea3bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üò¢', 'üò¢', 'üò¢', 'üò¢', 'üòä', 'üòä', 'üòä', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò≠', 'üò°', 'üò°', 'üò°', 'üò°', 'üò°', 'üò°', 'üò°', 'üò°', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ', 'üò∂', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'üòí', 'ü§†', 'ü§†', 'üòò', 'üòò', 'üòò', 'üòò', 'ü§î', 'ü§î', 'ü§î', 'ü§î', 'ü§î', 'ü§î', 'ü§î', 'ü§î', 'üòû', 'üòû', 'üòû', 'üòû', 'üòû', 'üòµ\\u200düí´', 'üòµ\\u200düí´', 'üò•', 'üò´', 'üíÉ', 'ü§ï', 'üòå', 'üòå', 'üòå', 'üò¨', 'üò¨', 'üò¨', 'üò¨', 'üò¨', 'üò¨', 'üò¨', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üòâ', 'üíÅüèª', '‚Ñ¢', '‚Ñ¢', '‚Ñ¢', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üíÄ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üôÑ', 'üß†', 'üß†', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÖ', 'üòÅ', 'üòÅ', 'üò≥', 'üò≥', 'üò≥', 'üò≥', 'üôä', 'üôä', 'üôä', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üôà', 'üòî', 'üòî', 'üòî', 'üòî', 'üòî', 'üòî', 'üòî', 'üòù', 'üòù', 'üòù', 'üòù', 'üòè', 'üòè', 'üòè', 'üòè', 'üòè', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'üôÉ', 'ü§£', 'ü§£', 'ü§£', 'ü§£', 'ü§£', 'ü§™', 'ü§™', 'ü§™', 'ü§™', 'üòï', 'üòï', 'üòï', 'üòÜ', 'üòÜ', 'üòÜ', 'ü§°', 'ü§ß', 'üëç', 'üëç', 'üëç', 'üëç', 'üòÄ', 'üòÉ', 'üí©', 'üí©', 'üò±', 'üò±', 'üê©', 'üë©', 'üòú', 'üòú', 'ü§Æ', 'ü§Æ', 'ü§Æ', 'ü§¢', 'ü™≥', 'üçΩ', 'ü§®', 'ü§®', 'üòê', 'üòê', 'üòë', 'üòë', 'üòë', 'üò∏', '‚úÖ', 'ü§¨', 'üßª', 'üò§', 'üò§', 'üò§', 'üò§', 'üò§', 'üò§', 'üë©\\u200düíª', 'üëâüèΩ', 'üëàüèΩ', 'üòé', 'üòé', 'ü§∑üèª\\u200d‚ôÄÔ∏è', 'ü§∑üèª\\u200d‚ôÄÔ∏è', 'üò£', 'ü•¥', 'ü•¥', 'ü•¥', 'ü§ó', 'üò©', 'üò©', 'üò©', 'üò©', 'üôèüèª', 'üñïüèº', '‚ú®', '‚ú®', '‚ú®', '‚ú®', '‚ú®', 'ü§Ø', 'üëå', 'üíî', 'üíî', 'üëçüèª', 'üî™', 'üëÄ', 'ü§¶üèΩ\\u200d‚ôÄÔ∏è', 'üíß', 'üí¶', '‚ÄºÔ∏è', 'ü§∑üèæ\\u200d‚ôÄÔ∏è', 'üòç', 'ü§¶üèº\\u200d‚ôÄÔ∏è', 'ü•≤', 'ü•≤', '‚ôã', 'üßê', 'üçã']\n"
     ]
    }
   ],
   "source": [
    "#Generate a list of the emojis in the verified data collection method tweets set- negative subset\n",
    "\n",
    "#Create an empty dictionary to store emoji counts\n",
    "sarc_emojis_count = {}\n",
    "\n",
    "#Iterate through each sarcastic tweet text\n",
    "for text in df_neg['tweet']:\n",
    "    emojis = demoji.findall(text)\n",
    "    \n",
    "    #Count the occurrence of each emoji in the current text\n",
    "    for emoji in emojis:\n",
    "        if emoji in sarc_emojis_count:\n",
    "            sarc_emojis_count[emoji] += 1\n",
    "        else:\n",
    "            sarc_emojis_count[emoji] = 1\n",
    "\n",
    "#Convert the dictionary into a list where emojis are repeated based on their count\n",
    "sarc_emojis_list_neg = []\n",
    "for emoji, count in sarc_emojis_count.items():\n",
    "    sarc_emojis_list_neg.extend([emoji] * count)\n",
    "    \n",
    "#Check the list\n",
    "print(sarc_emojis_list_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8834fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.3231850686804263\n",
      "Standard Deviation of Sentiment Scores: 0.4116289547140633\n",
      "Range at 95% confidence:\n",
      "-0.5000728407477003 1.1464429781085528\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the sent_score for negative sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "sent_score_pos_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "sent_score_pos_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_score_pos_ll, sent_score_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a7edd636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: -0.05139088463924955\n",
      "Standard Deviation of Sentiment Scores: 0.41334166688601154\n",
      "Range at 95% confidence:\n",
      "-0.8780742184112726 0.7752924491327735\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the sent_score for negative sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list_neg:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list_neg)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "sent_score_neg_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "sent_score_neg_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_score_neg_ll, sent_score_neg_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "80ced89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.4662691318820988\n",
      "Standard Deviation of Sentiment Scores: 0.2554795530026403\n",
      "Range at 95% confidence:\n",
      "-0.04468997412318182 0.9772282378873793\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_pos for positive sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_pos']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_pos']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_pos_pos_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_pos_pos_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_pos_pos_ll, pred_pos_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9abaaf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.26099296833755237\n",
      "Standard Deviation of Sentiment Scores: 0.19993162329082323\n",
      "Range at 95% confidence:\n",
      "-0.1388702782440941 0.6608562149191988\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_pos for negative sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list_neg:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_pos']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list_neg)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_pos']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_pos_neg_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_pos_neg_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_pos_neg_ll, pred_pos_neg_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0f068f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.1430840632016725\n",
      "Standard Deviation of Sentiment Scores: 0.177526346247982\n",
      "Range at 95% confidence:\n",
      "-0.2119686292942915 0.4981367556976365\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_neg for positive sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neg']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neg']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_neg_pos_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_neg_pos_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_neg_pos_ll, pred_neg_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2643e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.3123838529768019\n",
      "Standard Deviation of Sentiment Scores: 0.22883419474854796\n",
      "Range at 95% confidence:\n",
      "-0.14528453652029405 0.7700522424738978\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_neg for negative sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list_neg:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neg']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list_neg)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neg']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_neg_neg_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_neg_neg_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_neg_neg_ll, pred_neg_neg_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "425544e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.3906467958957859\n",
      "Standard Deviation of Sentiment Scores: 0.15534611150139568\n",
      "Range at 95% confidence:\n",
      "0.07995457289299451 0.7013390188985773\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_neut for positive sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neut']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neut']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_neut_pos_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_neut_pos_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_neut_pos_ll, pred_neut_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cc722582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Sentiment Score: 0.42662317094836183\n",
      "Standard Deviation of Sentiment Scores: 0.1175761521017841\n",
      "Range at 95% confidence:\n",
      "0.19147086674479363 0.66177547515193\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence of the pred_neut for negative sarcastic tweets\n",
    "\n",
    "#Create an empty list\n",
    "sarc_emoji_sentiment = {}\n",
    "\n",
    "#Find the relevant information for each emoji in the list in the df\n",
    "for emoji in sarc_emojis_list_neg:\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neut']\n",
    "        sarc_emoji_sentiment[emoji] = sentiment_score\n",
    "\n",
    "#Get all emojis and their counts\n",
    "all_emojis_sarc_count = Counter(sarc_emojis_list_neg)\n",
    "\n",
    "#Initialize variables for weighted average calculation\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "#Store sentiment scores for all emojis\n",
    "sentiment_scores = []\n",
    "\n",
    "#Calculate weighted average sentiment score and store sentiment scores\n",
    "for emoji, count in all_emojis_sarc_count.items():\n",
    "    emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "    \n",
    "    #If the emoji is found, get the sentiment score from the row\n",
    "    if not emoji_row.empty:\n",
    "        sentiment_score = emoji_row.iloc[0]['pred_neut']\n",
    "        weighted_score = sentiment_score * count\n",
    "        total_weighted_score += weighted_score\n",
    "        total_count += count\n",
    "        sentiment_scores.extend([sentiment_score] * count)\n",
    "\n",
    "#Calculate and print the weighted average sentiment score\n",
    "if total_count > 0:\n",
    "    weighted_average_sentiment = total_weighted_score / total_count\n",
    "    std_deviation = np.std(sentiment_scores)\n",
    "    print(f\"Weighted Average Sentiment Score: {weighted_average_sentiment}\")\n",
    "    print(f\"Standard Deviation of Sentiment Scores: {std_deviation}\")\n",
    "else:\n",
    "    print(\"No emojis found.\")\n",
    "    \n",
    "#Define upper and lower limits for the emoji sentiment score for positive sarcastic tweets\n",
    "pred_neut_neg_ul = weighted_average_sentiment + (2 * std_deviation)\n",
    "pred_neut_neg_ll = weighted_average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(pred_neut_neg_ll, pred_neut_neg_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "09735f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average emojis per string: 0.5691699604743083\n",
      "Standard deviation: 0.802760862731854\n",
      "Range at 95% confidence:\n",
      "-1.0363517649893996 2.1746916859380163\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence for the number of emojis used for positive sarcastic tweets\n",
    "\n",
    "#Make a list to hold number of emojis per tweet after editing\n",
    "emojis_per_string = []\n",
    "\n",
    "#Determin the number of emojis per tweet\n",
    "for text in df_pos['tweet']:\n",
    "    emoji_count = len(demoji.findall(text))\n",
    "    emojis_per_string.append(emoji_count)\n",
    "\n",
    "#Average and standard deviation\n",
    "average_emojis_per_string = np.mean(emojis_per_string)\n",
    "std_deviation = np.std(emojis_per_string)\n",
    "\n",
    "#Determine upper and lower limits\n",
    "emoji_number_pos_ul = average_emojis_per_string + (2 * std_deviation)\n",
    "emoji_number_pos_ll = average_emojis_per_string - (2 * std_deviation)\n",
    "\n",
    "#Print results\n",
    "print(\"Average emojis per string:\", average_emojis_per_string)\n",
    "print(\"Standard deviation:\", std_deviation)\n",
    "print('Range at 95% confidence:')\n",
    "print(emoji_number_pos_ll, emoji_number_pos_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "408f5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average emojis per string: 0.5066225165562914\n",
      "Standard deviation: 0.6796150911183374\n",
      "Range at 95% confidence:\n",
      "-0.8526076656803834 1.8658526987929662\n"
     ]
    }
   ],
   "source": [
    "#Determine the range at 95% confidence for the number of emojis used for negative sarcastic tweets\n",
    "\n",
    "#Make a list to hold number of emojis per tweet after editing\n",
    "emojis_per_string = []\n",
    "\n",
    "#Determin the number of emojis per tweet\n",
    "for text in df_neg['tweet']:\n",
    "    emoji_count = len(demoji.findall(text))\n",
    "    emojis_per_string.append(emoji_count)\n",
    "\n",
    "#Average and standard deviation\n",
    "average_emojis_per_string = np.mean(emojis_per_string)\n",
    "std_deviation = np.std(emojis_per_string)\n",
    "\n",
    "#Determine upper and lower limits\n",
    "emoji_number_neg_ul = average_emojis_per_string + (2 * std_deviation)\n",
    "emoji_number_neg_ll = average_emojis_per_string - (2 * std_deviation)\n",
    "\n",
    "#Print results\n",
    "print(\"Average emojis per string:\", average_emojis_per_string)\n",
    "print(\"Standard deviation:\", std_deviation)\n",
    "print('Range at 95% confidence:')\n",
    "print(emoji_number_neg_ll, emoji_number_neg_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9b765",
   "metadata": {},
   "source": [
    "# Text-Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "478cc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pos['sent_neg'] = df_pos['sentiment_result'].apply(lambda x: x[0])\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pos['sent_neut'] = df_pos['sentiment_result'].apply(lambda x: x[1])\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pos['sent_pos'] = df_pos['sentiment_result'].apply(lambda x: x[2])\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_neg['sent_neg'] = df_neg['sentiment_result'].apply(lambda x: x[0])\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_neg['sent_neut'] = df_neg['sentiment_result'].apply(lambda x: x[1])\n",
      "C:\\Users\\niamh\\AppData\\Local\\Temp\\ipykernel_88480\\3643696327.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_neg['sent_pos'] = df_neg['sentiment_result'].apply(lambda x: x[2])\n"
     ]
    }
   ],
   "source": [
    "#Extract positivity, negativity and neutrality probabilities for the datasets\n",
    "df_pos['sent_neg'] = df_pos['sentiment_result'].apply(lambda x: x[0])\n",
    "df_pos['sent_neut'] = df_pos['sentiment_result'].apply(lambda x: x[1])\n",
    "df_pos['sent_pos'] = df_pos['sentiment_result'].apply(lambda x: x[2])\n",
    "\n",
    "df_neg['sent_neg'] = df_neg['sentiment_result'].apply(lambda x: x[0])\n",
    "df_neg['sent_neut'] = df_neg['sentiment_result'].apply(lambda x: x[1])\n",
    "df_neg['sent_pos'] = df_neg['sentiment_result'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "46a4d519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_result</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sent_neg</th>\n",
       "      <th>sent_neut</th>\n",
       "      <th>sent_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college was a caffei...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1715908795595169, 0.5636150240898132, 0.264...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.171591</td>\n",
       "      <td>0.563615</td>\n",
       "      <td>0.264794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My (extended) fam was discussing going on a t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.21971991658210754, 0.37854474782943726, 0.4...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.219720</td>\n",
       "      <td>0.378545</td>\n",
       "      <td>0.401735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i love shoegaze sm üòÅ</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0015152746345847845, 0.00982738845050335, 0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.988657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay but like the say so song aint that bad. I...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02680261805653572, 0.2406616508960724, 0.73...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.240662</td>\n",
       "      <td>0.732536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tinnitus is my favourite thing ever I love it....</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0021385003346949816, 0.0065424032509326935,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.991319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label   \n",
       "0  The only thing I got from college was a caffei...      1  \\\n",
       "1  \"My (extended) fam was discussing going on a t...      1   \n",
       "2                               i love shoegaze sm üòÅ      1   \n",
       "4  okay but like the say so song aint that bad. I...      1   \n",
       "5  Tinnitus is my favourite thing ever I love it....      1   \n",
       "\n",
       "                                    sentiment_result sentiment_label   \n",
       "0  [0.1715908795595169, 0.5636150240898132, 0.264...        positive  \\\n",
       "1  [0.21971991658210754, 0.37854474782943726, 0.4...        positive   \n",
       "2  [0.0015152746345847845, 0.00982738845050335, 0...        positive   \n",
       "4  [0.02680261805653572, 0.2406616508960724, 0.73...        positive   \n",
       "5  [0.0021385003346949816, 0.0065424032509326935,...        positive   \n",
       "\n",
       "   sent_neg  sent_neut  sent_pos  \n",
       "0  0.171591   0.563615  0.264794  \n",
       "1  0.219720   0.378545  0.401735  \n",
       "2  0.001515   0.009827  0.988657  \n",
       "4  0.026803   0.240662  0.732536  \n",
       "5  0.002139   0.006542  0.991319  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check df\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0f885967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "0.10248569569120758 1.2313520689630275\n",
      "Average Sentiment Score: 0.6669188823271175\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_pos positive sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_pos['sent_pos']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_pos['sent_pos'])\n",
    "\n",
    "#Define upper and lower limits\n",
    "sent_pos_pos_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_pos_pos_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_pos_pos_ll, sent_pos_pos_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fde6ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "-0.06880150744886523 0.21023646793978126\n",
      "Average Sentiment Score: 0.07071748024545801\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_pos negative sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_neg['sent_pos']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_neg['sent_pos'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "sent_pos_neg_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_pos_neg_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_pos_neg_ll, sent_pos_neg_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "81370001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "-0.08705946498226866 0.19108284018581637\n",
      "Average Sentiment Score: 0.05201168760177385\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_neg positive sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_pos['sent_neg']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_pos['sent_neg'])\n",
    "\n",
    "#Define upper and lower limits\n",
    "sent_neg_pos_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_neg_pos_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_neg_pos_ll, sent_neg_pos_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "03f66ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "0.09743568282562393 1.0801792006385758\n",
      "Average Sentiment Score: 0.5888074417320999\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_neg negative sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_neg['sent_neg']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_neg['sent_neg'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "sent_neg_neg_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_neg_neg_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_neg_neg_ll, sent_neg_neg_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2996d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "-0.2127798265007919 0.7749186871852838\n",
      "Average Sentiment Score: 0.28106943034224596\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_neut positive sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_pos['sent_neut']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_pos['sent_neut'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "sent_neut_pos_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_neut_pos_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_neut_pos_ll, sent_neut_pos_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "11d4e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range at 95% confidence:\n",
      "-0.07048796082615533 0.7514381175598943\n",
      "Average Sentiment Score: 0.3404750783668695\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for sent_neut negative sarcastic text\n",
    "\n",
    "#Calculate average sentiment score\n",
    "average_sentiment = (df_neg['sent_neut']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation = np.std(df_neg['sent_neut'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "sent_neut_neg_ul = average_sentiment + (2 * std_deviation)\n",
    "sent_neut_neg_ll = average_sentiment - (2 * std_deviation)\n",
    "\n",
    "#Print limits\n",
    "print('Range at 95% confidence:')\n",
    "print(sent_neut_neg_ll, sent_neut_neg_ul)\n",
    "\n",
    "print(f\"Average Sentiment Score: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "320725e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-1.1101739383252867 1.438237179432006\n",
      "Average: 0.16403162055335968\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-0.7585752180032551 0.967184489526434\n",
      "Average: 0.10430463576158941\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for hashtag use for sarcastic positive/negative tweets\n",
    "\n",
    "#Define function to count hashtags\n",
    "def count_hashtags(text):\n",
    "    words = text.split()\n",
    "    hashtag_count = sum(1 for word in words if word.startswith('#'))\n",
    "    return hashtag_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['hashtags'] = df_pos['tweet'].apply(count_hashtags)\n",
    "df_neg['hashtags'] = df_neg['tweet'].apply(count_hashtags)\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['hashtags']).mean()\n",
    "average_neg = (df_neg['hashtags']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['hashtags'])\n",
    "std_deviation_neg = np.std(df_neg['hashtags'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "hashtags_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "hashtags_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "hashtags_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "hashtags_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(hashtags_pos_ll, hashtags_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(hashtags_neg_ll, hashtags_neg_ul)\n",
    "print(f\"Average: {average_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "af00ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-0.3874770174995582 0.4546706933888862\n",
      "Average: 0.03359683794466403\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-0.20246750144230585 0.22564630938932573\n",
      "Average: 0.011589403973509934\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for laughter use for sarcastic positive/negative tweets\n",
    "\n",
    "#Define the function to count laughter occurrences\n",
    "def count_laughter(text):\n",
    "    words = text.split()\n",
    "    laugh_count = sum(1 for word in words if word.startswith(\"haha\") or re.match('l(o)+l$', word, re.IGNORECASE))\n",
    "    return laugh_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['laughter'] = df_pos['tweet'].apply(count_laughter)\n",
    "df_neg['laughter'] = df_neg['tweet'].apply(count_laughter)\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['laughter']).mean()\n",
    "average_neg = (df_neg['laughter']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['laughter'])\n",
    "std_deviation_neg = np.std(df_neg['laughter'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "laughter_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "laughter_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "laughter_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "laughter_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(laughter_pos_ll, laughter_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(laughter_neg_ll, laughter_neg_ul)\n",
    "print(f\"Average: {average_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4330e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-1.1861548562741777 2.190107425444138\n",
      "Average: 0.5019762845849802\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-1.1233772597189633 2.189602425281877\n",
      "Average: 0.5331125827814569\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use of affirmatives for sarcastic positive/negative tweets\n",
    "\n",
    "#Make list of strong affirmatives\n",
    "strong_affirmatives = [\"yes\", \"yeah\", \"always\", \"all\", \"any\", \"every\", \"everybody\", \"everywhere\", \"ever\"]\n",
    "\n",
    "#Define the function to count strong affermatives\n",
    "def count_affirmatives(text, strong_affirmatives):\n",
    "    words = text.split()\n",
    "    affirmatives_count = sum(text.count(n) for n in strong_affirmatives)\n",
    "    return affirmatives_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['affirmatives'] = df_pos['tweet'].apply(lambda x: count_affirmatives(x, strong_affirmatives))\n",
    "df_neg['affirmatives'] = df_neg['tweet'].apply(lambda x: count_affirmatives(x, strong_affirmatives))\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['affirmatives']).mean()\n",
    "average_neg = (df_neg['affirmatives']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['affirmatives'])\n",
    "std_deviation_neg = np.std(df_neg['affirmatives'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "affirmatives_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "affirmatives_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "affirmatives_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "affirmatives_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(affirmatives_pos_ll, affirmatives_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(affirmatives_neg_ll, affirmatives_neg_ul)\n",
    "print(f\"Average: {average_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bb98064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-1.1787682222046671 1.9376615028370783\n",
      "Average: 0.3794466403162055\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-1.4765654238505217 2.7778899271617803\n",
      "Average: 0.6506622516556292\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use use of negations for sarcastic positive/negative tweets\n",
    "\n",
    "#Make list of strong negations\n",
    "strong_negations = [\"no\", \"not\", \"never\", \"none\", \"n't\", \"nothing\", \"neither\", \"nobody\", \"nowhere\"]\n",
    "\n",
    "#Define the function to count strong negations\n",
    "def count_negation(text, strong_negations):\n",
    "    words = text.split()\n",
    "    negations_count = sum(text.count(n) for n in strong_negations)\n",
    "    return negations_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['negations'] = df_pos['tweet'].apply(lambda x: count_negation(x, strong_negations))\n",
    "df_neg['negations'] = df_neg['tweet'].apply(lambda x: count_negation(x, strong_negations))\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['negations']).mean()\n",
    "average_neg = (df_neg['negations']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['negations'])\n",
    "std_deviation_neg = np.std(df_neg['negations'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "negations_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "negations_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "negations_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "negations_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(negations_pos_ll, negations_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(negations_neg_ll, negations_neg_ul)\n",
    "print(f\"Average: {average_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e616adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-0.7192766513144375 1.2133477975594968\n",
      "Average: 0.24703557312252963\n",
      "Standard Deviation: 0.4831561122184836\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-1.0356827495542464 1.6747555972363657\n",
      "Average: 0.3195364238410596\n",
      "Standard Deviation: 0.677609586697653\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use use of intensifiers for sarcastic positive/negative tweets\n",
    "\n",
    "#Make a list of intensifiers\n",
    "intensifiers = [\"amazingly\", \"astoundingly\", \"awful\", \"bare\", \"bloody\", \"crazy\", \"dreadfully\",\n",
    "                \"colossally\", \"especially\", \"exceptionally\", \"excessively\", \"extremely\",\n",
    "                \"extraordinarily\", \"fantastically\", \"frightfully\", \"fucking\", \"fully\", \"hella\",\n",
    "                \"holy\", \"incredibly\", \"insanely\", \"literally\", \"mightily\", \"moderately\", \"most\",\n",
    "                \"outrageously\", \"phenomenally\", \"precious\", \"quite\", \"radically\", \"rather\",\n",
    "                \"really\", \"remarkably\", \"right\", \"sick\", \"strikingly\", \"super\", \"supremely\",\n",
    "                \"surprisingly\", \"terribly\", \"terrifically\", \"too\", \"totally\", \"uncommonly\",\n",
    "                \"unusually\", \"veritable\", \"very\", \"wicked\"]\n",
    "\n",
    "#Define the function to count intensifiers\n",
    "def count_intensifiers(text, intensifiers):\n",
    "    words = text.split()\n",
    "    intensifier_count = sum(text.count(n) for n in intensifiers)\n",
    "    return intensifier_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['intensifiers'] = df_pos['tweet'].apply(lambda x: count_intensifiers(x, intensifiers))\n",
    "df_neg['intensifiers'] = df_neg['tweet'].apply(lambda x: count_intensifiers(x, intensifiers))\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['intensifiers']).mean()\n",
    "average_neg = (df_neg['intensifiers']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['intensifiers'])\n",
    "std_deviation_neg = np.std(df_neg['intensifiers'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "intensifiers_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "intensifiers_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "intensifiers_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "intensifiers_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(intensifiers_pos_ll, intensifiers_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(intensifiers_neg_ll, intensifiers_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5fa80ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-2.405454262257586 4.567509598225965\n",
      "Average: 1.0810276679841897\n",
      "Standard Deviation: 1.7432409651208878\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-2.0262431227957585 4.777898751934831\n",
      "Average: 1.3758278145695364\n",
      "Standard Deviation: 1.7010354686826474\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use use of interjections for sarcastic positive/negative tweets\n",
    "\n",
    "#Make a list of interjections\n",
    "interjections = [\"oh\", \"hey\", \"wow\", \"aha\", \"aham\", \"aw\", \"bam\", \"blah\", \"bingo\", \"boo\", \"bravo\",\n",
    "                 \"cheers\", \"congratulations\", \"congrats\", \"duh\", \"eh\", \"gee\", \"gosh\", \"hey\", \"hmm\",\n",
    "                 \"huh\", \"hurray\", \"oh\", \"oh dear\", \"oh my\", \"oh well\", \"oops\", \"ouch\", \"ow\", \"phew\",\n",
    "                 \"shh\", \"uh\", \"uh-huh\", \"mhm\", \"ugh\", \"well\", \"wow\", \"woah\", \"yeah\", \"yep\", \"yikes\", \"yo\"]\n",
    "\n",
    "#Define the function to count interjections\n",
    "def count_interjections(text, interjections):\n",
    "    words = text.split()\n",
    "    interjection_count = sum(text.count(n) for n in interjections)\n",
    "    return interjection_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['interjections'] = df_pos['tweet'].apply(lambda x: count_interjections(x, interjections))\n",
    "df_neg['interjections'] = df_neg['tweet'].apply(lambda x: count_interjections(x, interjections))\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['interjections']).mean()\n",
    "average_neg = (df_neg['interjections']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['interjections'])\n",
    "std_deviation_neg = np.std(df_neg['interjections'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "interjections_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "interjections_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "interjections_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "interjections_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(interjections_pos_ll, interjections_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(interjections_neg_ll, interjections_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "999c0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-1.6460326968954686 2.4918824992670103\n",
      "Average: 0.42292490118577075\n",
      "Standard Deviation: 1.0344787990406197\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-2.0587821191950484 3.1150735099235254\n",
      "Average: 0.5281456953642384\n",
      "Standard Deviation: 1.2934639072796434\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use of potentially relevant punctuation for sarcastic positive/negative tweets\n",
    "\n",
    "#Define list of potentially relevant punctuation\n",
    "punctuation = [\"?\", \"!\", \"...\"]\n",
    "\n",
    "#Define the function to count relevant punctuation\n",
    "def count_punctuation(text, punctuation):\n",
    "    words = text.split()\n",
    "    punctuation_count = sum(text.count(p) for p in punctuation)\n",
    "    return punctuation_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['punctuation'] = df_pos['tweet'].apply(lambda x: count_punctuation(x, punctuation))\n",
    "df_neg['punctuation'] = df_neg['tweet'].apply(lambda x: count_punctuation(x, punctuation))\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['punctuation']).mean()\n",
    "average_neg = (df_neg['punctuation']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['punctuation'])\n",
    "std_deviation_neg = np.std(df_neg['punctuation'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "punctuation_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "punctuation_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "punctuation_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "punctuation_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(punctuation_pos_ll, punctuation_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(punctuation_neg_ll, punctuation_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d06d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-0.9417058693646669 1.3962513239101215\n",
      "Average: 0.22727272727272727\n",
      "Standard Deviation: 0.5844892983186971\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-1.2504351986615267 1.7669914900522552\n",
      "Average: 0.2582781456953642\n",
      "Standard Deviation: 0.7543566721784455\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use of user mentions for sarcastic positive/negative tweets\n",
    "\n",
    "#Define function to count user mentions\n",
    "def count_user_mentions(text):\n",
    "    words = text.split()\n",
    "    user_mentions_count = sum(1 for word in words if word.startswith('@'))\n",
    "    return user_mentions_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['user_mentions'] = df_pos['tweet'].apply(count_user_mentions)\n",
    "df_neg['user_mentions'] = df_neg['tweet'].apply(count_user_mentions)\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['user_mentions']).mean()\n",
    "average_neg = (df_neg['user_mentions']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['user_mentions'])\n",
    "std_deviation_neg = np.std(df_neg['user_mentions'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "user_mentions_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "user_mentions_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "user_mentions_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "user_mentions_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(user_mentions_pos_ll, user_mentions_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(user_mentions_neg_ll, user_mentions_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d3242d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-2.3756422453496517 6.375642245349652\n",
      "Average: 2.0\n",
      "Standard Deviation: 2.187821122674826\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-2.4294763439934215 6.6877544896887855\n",
      "Average: 2.129139072847682\n",
      "Standard Deviation: 2.2793077084205517\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use of capitalised words for sarcastic positive/negative tweets\n",
    "\n",
    "#Define function to count capitalised words\n",
    "def count_capital_words(text):\n",
    "    words = text.split()\n",
    "    capital_words_count = sum(1 for word in words if word[0].isupper())\n",
    "    return capital_words_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['capital_words'] = df_pos['tweet'].apply(count_capital_words)\n",
    "df_neg['capital_words'] = df_neg['tweet'].apply(count_capital_words)\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['capital_words']).mean()\n",
    "average_neg = (df_neg['capital_words']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['capital_words'])\n",
    "std_deviation_neg = np.std(df_neg['capital_words'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "capital_words_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "capital_words_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "capital_words_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "capital_words_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(capital_words_pos_ll, capital_words_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(capital_words_neg_ll, capital_words_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "91185aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive subset:\n",
      "Range at 95% confidence:\n",
      "-2.1441905335625226 3.2469573319814953\n",
      "Average: 0.5513833992094862\n",
      "Standard Deviation: 1.3477869663860045\n",
      "\n",
      "Negative subset:\n",
      "Range at 95% confidence:\n",
      "-2.733522175469494 3.9388201887145264\n",
      "Average: 0.6026490066225165\n",
      "Standard Deviation: 1.668085591046005\n"
     ]
    }
   ],
   "source": [
    "#Determine mean and range for use of mid-word capitalisations for sarcastic positive/negative tweets\n",
    "\n",
    "#Define the function to count capitalisation in the middle of words\n",
    "def count_midword_capitalisation(text):\n",
    "    words = text.split()\n",
    "    midword_capitalisation_count = sum(1 for word in words if any(char.isupper() for char in word[1:]))\n",
    "    return midword_capitalisation_count\n",
    "\n",
    "#Apply to tweets column\n",
    "df_pos['midword_caps'] = df_pos['tweet'].apply(count_midword_capitalisation)\n",
    "df_neg['midword_caps'] = df_neg['tweet'].apply(count_midword_capitalisation)\n",
    "\n",
    "#Calculate average\n",
    "average_pos = (df_pos['midword_caps']).mean()\n",
    "average_neg = (df_neg['midword_caps']).mean()\n",
    "#Calculate standard deviation\n",
    "std_deviation_pos = np.std(df_pos['midword_caps'])\n",
    "std_deviation_neg = np.std(df_neg['midword_caps'])\n",
    "\n",
    "#Define upper and lower limits \n",
    "midword_caps_pos_ul = average_pos + (2 * std_deviation_pos)\n",
    "midword_caps_pos_ll = average_pos - (2 * std_deviation_pos)\n",
    "midword_caps_neg_ul = average_neg + (2 * std_deviation_neg)\n",
    "midword_caps_neg_ll = average_neg - (2 * std_deviation_neg)\n",
    "\n",
    "#Print results\n",
    "print('Positive subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(midword_caps_pos_ll, midword_caps_pos_ul)\n",
    "print(f\"Average: {average_pos}\")\n",
    "print(f'Standard Deviation: {std_deviation_pos}')\n",
    "print()\n",
    "print('Negative subset:')\n",
    "print('Range at 95% confidence:')\n",
    "print(midword_caps_neg_ll, midword_caps_neg_ul)\n",
    "print(f\"Average: {average_neg}\")\n",
    "print(f'Standard Deviation: {std_deviation_neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04570fd7",
   "metadata": {},
   "source": [
    "# Check for each dataset which tweets fall within reasonable limits- Emoji Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "62582d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to check if a value is within the limits of the previously determined metrics\n",
    "def check_limits(value, lower_limit, upper_limit):\n",
    "    if lower_limit <= value <= upper_limit:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9dab8a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emojis_per_tweet\n",
       "Yes    38273\n",
       "No       526\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define function to count the emojis in each string\n",
    "def count_emojis(text):\n",
    "    return len(demoji.findall(text))\n",
    "\n",
    "#Apply the function for each dataset under evaluation to check if number of emojis used are within typical limits\n",
    "add_pos['emojis_per_tweet'] = add_pos['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_pos_ll, \n",
    "                                                                   emoji_number_pos_ul))\n",
    "add_neg['emojis_per_tweet'] = add_neg['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_neg_ll, \n",
    "                                                                   emoji_number_neg_ul))\n",
    "\n",
    "add1_pos['emojis_per_tweet'] = add1_pos['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_pos_ll, \n",
    "                                                                   emoji_number_pos_ul))\n",
    "add1_neg['emojis_per_tweet'] = add1_neg['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_neg_ll, \n",
    "                                                                   emoji_number_neg_ul))\n",
    "\n",
    "add2_pos['emojis_per_tweet'] = add2_pos['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_pos_ll, \n",
    "                                                                   emoji_number_pos_ul))\n",
    "add2_neg['emojis_per_tweet'] = add2_neg['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_neg_ll, \n",
    "                                                                   emoji_number_neg_ul))\n",
    "\n",
    "add3_pos['emojis_per_tweet'] = add3_pos['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_pos_ll, \n",
    "                                                                   emoji_number_pos_ul))\n",
    "add3_neg['emojis_per_tweet'] = add3_neg['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_neg_ll, \n",
    "                                                                   emoji_number_neg_ul))\n",
    "\n",
    "add4_pos['emojis_per_tweet'] = add4_pos['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_pos_ll, \n",
    "                                                                   emoji_number_pos_ul))\n",
    "add4_neg['emojis_per_tweet'] = add4_neg['text'].apply(lambda x: check_limits(count_emojis(x), emoji_number_neg_ll, \n",
    "                                                                   emoji_number_neg_ul))\n",
    "\n",
    "#Check one result\n",
    "add_pos['emojis_per_tweet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0c9e1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_score\n",
      "Yes    3435\n",
      "No      226\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Define function to calculate the average sent_score of the emojis in a string\n",
    "def calculate_avg_sentiment(text, emotion_info):\n",
    "    emoji_count = {}\n",
    "    #Find all of the emojis\n",
    "    emojis = demoji.findall(text)\n",
    "    #Account for emojis that appear several times\n",
    "    for emoji in emojis:\n",
    "        if emoji in emoji_count:\n",
    "            emoji_count[emoji] += 1\n",
    "        else:\n",
    "            emoji_count[emoji] = 1\n",
    "    #Convert dictionary to a list where emojis are repeated based on their count\n",
    "    emoji_list = []\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_list.extend([emoji]*count)\n",
    "    #Initialize variables for weighted average calculation\n",
    "    total_weighted_score = 0\n",
    "    total_count = 0\n",
    "    #Calculate weighted average sentiment score\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "        #If the emoji is found, get the sentiment score from the row\n",
    "        if not emoji_row.empty:\n",
    "            sentiment_score = emoji_row.iloc[0]['pred_sent_score']\n",
    "            weighted_score = sentiment_score * count\n",
    "            total_weighted_score += weighted_score\n",
    "            total_count += count\n",
    "    if total_count > 0:\n",
    "        weighted_average_sentiment = total_weighted_score / total_count\n",
    "        return weighted_average_sentiment\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "#Apply function to each dataset being evaluated\n",
    "add_pos['sent_score'] = add_pos[add_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_pos_ll, sent_score_pos_ul), axis=1)\n",
    "add_neg['sent_score'] = add_neg[add_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_neg_ll, sent_score_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add1_pos['sent_score'] = add1_pos[add1_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_pos_ll, sent_score_pos_ul), axis=1)\n",
    "add1_neg['sent_score'] = add1_neg[add1_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_neg_ll, sent_score_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add2_pos['sent_score'] = add2_pos[add2_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_pos_ll, sent_score_pos_ul), axis=1)\n",
    "add2_neg['sent_score'] = add2_neg[add2_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_neg_ll, sent_score_neg_ul), axis=1)\n",
    "\n",
    "add3_pos['sent_score'] = add3_pos[add3_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_pos_ll, sent_score_pos_ul), axis=1)\n",
    "add3_neg['sent_score'] = add3_neg[add3_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_neg_ll, sent_score_neg_ul), axis=1)\n",
    "\n",
    "add4_pos['sent_score'] = add4_pos[add4_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_pos_ll, sent_score_pos_ul), axis=1)\n",
    "add4_neg['sent_score'] = add4_neg[add4_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   sent_score_neg_ll, sent_score_neg_ul), axis=1)\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['sent_score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c1605292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_pos\n",
      "Yes    3524\n",
      "No      137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Define function to calculate the average pred_pos of the emojis in a string\n",
    "def calculate_avg_sentiment(text, emotion_info):\n",
    "    emoji_count = {}\n",
    "    #Find all of the emojis\n",
    "    emojis = demoji.findall(text)\n",
    "    #Account for emojis that appear several times\n",
    "    for emoji in emojis:\n",
    "        if emoji in emoji_count:\n",
    "            emoji_count[emoji] += 1\n",
    "        else:\n",
    "            emoji_count[emoji] = 1\n",
    "    #Convert dictionary to a list where emojis are repeated based on their count\n",
    "    emoji_list = []\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_list.extend([emoji]*count)\n",
    "    #Initialize variables for weighted average calculation\n",
    "    total_weighted_score = 0\n",
    "    total_count = 0\n",
    "    #Calculate weighted average sentiment score\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "        #If the emoji is found, get the sentiment score from the row\n",
    "        if not emoji_row.empty:\n",
    "            sentiment_score = emoji_row.iloc[0]['pred_pos']\n",
    "            weighted_score = sentiment_score * count\n",
    "            total_weighted_score += weighted_score\n",
    "            total_count += count\n",
    "    if total_count > 0:\n",
    "        weighted_average_sentiment = total_weighted_score / total_count\n",
    "        return weighted_average_sentiment\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "#Apply function to each dataset being evaluated\n",
    "add_pos['pred_pos'] = add_pos[add_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_pos_ll, pred_pos_pos_ul), axis=1)\n",
    "add_neg['pred_pos'] = add_neg[add_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_neg_ll, pred_pos_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add1_pos['pred_pos'] = add1_pos[add1_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_pos_ll, pred_pos_pos_ul), axis=1)\n",
    "add1_neg['pred_pos'] = add1_neg[add1_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_neg_ll, pred_pos_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add2_pos['pred_pos'] = add2_pos[add2_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_pos_ll, pred_pos_pos_ul), axis=1)\n",
    "add2_neg['pred_pos'] = add2_neg[add2_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_neg_ll, pred_pos_neg_ul), axis=1)\n",
    "\n",
    "add3_pos['pred_pos'] = add3_pos[add3_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_pos_ll, pred_pos_pos_ul), axis=1)\n",
    "add3_neg['pred_pos'] = add3_neg[add3_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_neg_ll, pred_pos_neg_ul), axis=1)\n",
    "\n",
    "add4_pos['pred_pos'] = add4_pos[add4_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_pos_ll, pred_pos_pos_ul), axis=1)\n",
    "add4_neg['pred_pos'] = add4_neg[add4_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_pos_neg_ll, pred_pos_neg_ul), axis=1)\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['pred_pos'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8124b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_neut\n",
      "Yes    3498\n",
      "No      163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Define function to calculate the average pred_neut of the emojis in a string\n",
    "def calculate_avg_sentiment(text, emotion_info):\n",
    "    emoji_count = {}\n",
    "    #Find all of the emojis\n",
    "    emojis = demoji.findall(text)\n",
    "    #Account for emojis that appear several times\n",
    "    for emoji in emojis:\n",
    "        if emoji in emoji_count:\n",
    "            emoji_count[emoji] += 1\n",
    "        else:\n",
    "            emoji_count[emoji] = 1\n",
    "    #Convert dictionary to a list where emojis are repeated based on their count\n",
    "    emoji_list = []\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_list.extend([emoji]*count)\n",
    "    #Initialize variables for weighted average calculation\n",
    "    total_weighted_score = 0\n",
    "    total_count = 0\n",
    "    #Calculate weighted average sentiment score\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "        #If the emoji is found, get the sentiment score from the row\n",
    "        if not emoji_row.empty:\n",
    "            sentiment_score = emoji_row.iloc[0]['pred_neut']\n",
    "            weighted_score = sentiment_score * count\n",
    "            total_weighted_score += weighted_score\n",
    "            total_count += count\n",
    "    if total_count > 0:\n",
    "        weighted_average_sentiment = total_weighted_score / total_count\n",
    "        return weighted_average_sentiment\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "#Apply function to each dataset being evaluated\n",
    "add_pos['pred_neut'] = add_pos[add_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_pos_ll, pred_neut_pos_ul), axis=1)\n",
    "add_neg['pred_neut'] = add_neg[add_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_neg_ll, pred_neut_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add1_pos['pred_neut'] = add1_pos[add1_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_pos_ll, pred_neut_pos_ul), axis=1)\n",
    "add1_neg['pred_neut'] = add1_neg[add1_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_neg_ll, pred_neut_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add2_pos['pred_neut'] = add2_pos[add2_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_pos_ll, pred_neut_pos_ul), axis=1)\n",
    "add2_neg['pred_neut'] = add2_neg[add2_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_neg_ll, pred_neut_neg_ul), axis=1)\n",
    "\n",
    "add3_pos['pred_neut'] = add3_pos[add3_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_pos_ll, pred_neut_pos_ul), axis=1)\n",
    "add3_neg['pred_neut'] = add3_neg[add3_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_neg_ll, pred_neut_neg_ul), axis=1)\n",
    "\n",
    "add4_pos['pred_neut'] = add4_pos[add4_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_pos_ll, pred_neut_pos_ul), axis=1)\n",
    "add4_neg['pred_neut'] = add4_neg[add4_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neut_neg_ll, pred_neut_neg_ul), axis=1)\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['pred_neut'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cf65b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_neg\n",
      "Yes    3275\n",
      "No      386\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Define function to calculate the average pred_neg of the emojis in a string\n",
    "def calculate_avg_sentiment(text, emotion_info):\n",
    "    emoji_count = {}\n",
    "    #Find all of the emojis\n",
    "    emojis = demoji.findall(text)\n",
    "    #Account for emojis that appear several times\n",
    "    for emoji in emojis:\n",
    "        if emoji in emoji_count:\n",
    "            emoji_count[emoji] += 1\n",
    "        else:\n",
    "            emoji_count[emoji] = 1\n",
    "    #Convert dictionary to a list where emojis are repeated based on their count\n",
    "    emoji_list = []\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_list.extend([emoji]*count)\n",
    "    #Initialize variables for weighted average calculation\n",
    "    total_weighted_score = 0\n",
    "    total_count = 0\n",
    "    #Calculate weighted average sentiment score\n",
    "    for emoji, count in emoji_count.items():\n",
    "        emoji_row = emotion_info[emotion_info['emoji'] == emoji]\n",
    "        #If the emoji is found, get the sentiment score from the row\n",
    "        if not emoji_row.empty:\n",
    "            sentiment_score = emoji_row.iloc[0]['pred_neg']\n",
    "            weighted_score = sentiment_score * count\n",
    "            total_weighted_score += weighted_score\n",
    "            total_count += count\n",
    "    if total_count > 0:\n",
    "        weighted_average_sentiment = total_weighted_score / total_count\n",
    "        return weighted_average_sentiment\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "#Apply function to each dataset being evaluated\n",
    "add_pos['pred_neg'] = add_pos[add_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_pos_ll, pred_neg_pos_ul), axis=1)\n",
    "add_neg['pred_neg'] = add_neg[add_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_neg_ll, pred_neg_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add1_pos['pred_neg'] = add1_pos[add1_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_pos_ll, pred_neg_pos_ul), axis=1)\n",
    "add1_neg['pred_neg'] = add1_neg[add1_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_neg_ll, pred_neg_neg_ul), axis=1)\n",
    "\n",
    "\n",
    "add2_pos['pred_neg'] = add2_pos[add2_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_pos_ll, pred_neg_pos_ul), axis=1)\n",
    "add2_neg['pred_neg'] = add2_neg[add2_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_neg_ll, pred_neg_neg_ul), axis=1)\n",
    "\n",
    "add3_pos['pred_neg'] = add3_pos[add3_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_pos_ll, pred_neg_pos_ul), axis=1)\n",
    "add3_neg['pred_neg'] = add3_neg[add3_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_neg_ll, pred_neg_neg_ul), axis=1)\n",
    "\n",
    "add4_pos['pred_neg'] = add4_pos[add4_pos['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_pos_ll, pred_neg_pos_ul), axis=1)\n",
    "add4_neg['pred_neg'] = add4_neg[add4_neg['text'].apply(lambda x: demoji.findall(x) != {})]\\\n",
    "    .apply(lambda row: check_limits(calculate_avg_sentiment(row['text'], emotion_info), \n",
    "                                   pred_neg_neg_ll, pred_neg_neg_ul), axis=1)\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['pred_neg'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc57e3",
   "metadata": {},
   "source": [
    "# Check for each dataset which tweets fall within reasonable limits- Test-Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cfec362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags\n",
      "Yes    21780\n",
      "No     17019\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check number of hashtags using previously defined function\n",
    "add_pos['hashtags'] = add_pos['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_pos_ll, hashtags_pos_ul))\n",
    "add_neg['hashtags'] = add_neg['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_neg_ll, hashtags_neg_ul))\n",
    "\n",
    "add1_pos['hashtags'] = add1_pos['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_pos_ll, hashtags_pos_ul))\n",
    "add1_neg['hashtags'] = add1_neg['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_neg_ll, hashtags_neg_ul))\n",
    "\n",
    "add2_pos['hashtags'] = add2_pos['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_pos_ll, hashtags_pos_ul))\n",
    "add2_neg['hashtags'] = add2_neg['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_neg_ll, hashtags_neg_ul))\n",
    "\n",
    "add3_pos['hashtags'] = add3_pos['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_pos_ll, hashtags_pos_ul))\n",
    "add3_neg['hashtags'] = add3_neg['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_neg_ll, hashtags_neg_ul))\n",
    "\n",
    "add4_pos['hashtags'] = add4_pos['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_pos_ll, hashtags_pos_ul))\n",
    "add4_neg['hashtags'] = add4_neg['text'].apply(lambda x: check_limits(count_hashtags(x), \n",
    "                                                                    hashtags_neg_ll, hashtags_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['hashtags'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c53c07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughter\n",
      "Yes    38096\n",
      "No       703\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check number of laughter indicators using previously defined function\n",
    "add_pos['laughter'] = add_pos['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_pos_ll, laughter_pos_ul))\n",
    "add_neg['laughter'] = add_neg['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_neg_ll, laughter_neg_ul))\n",
    "\n",
    "add1_pos['laughter'] = add1_pos['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_pos_ll, laughter_pos_ul))\n",
    "add1_neg['laughter'] = add1_neg['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_neg_ll, laughter_neg_ul))\n",
    "\n",
    "add2_pos['laughter'] = add2_pos['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_pos_ll, laughter_pos_ul))\n",
    "add2_neg['laughter'] = add2_neg['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_neg_ll, laughter_neg_ul))\n",
    "\n",
    "add3_pos['laughter'] = add3_pos['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_pos_ll, laughter_pos_ul))\n",
    "add3_neg['laughter'] = add3_neg['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_neg_ll, laughter_neg_ul))\n",
    "\n",
    "add4_pos['laughter'] = add4_pos['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_pos_ll, laughter_pos_ul))\n",
    "add4_neg['laughter'] = add4_neg['text'].apply(lambda x: check_limits(count_laughter(x), \n",
    "                                                                    laughter_neg_ll, laughter_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['laughter'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a308ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capitalised_words\n",
      "Yes    36758\n",
      "No      2041\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check number of capitalised words using previously defined function\n",
    "add_pos['capitalised_words'] = add_pos['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_pos_ll, capital_words_pos_ul))\n",
    "add_neg['capitalised_words'] = add_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_neg_ll, capital_words_neg_ul))\n",
    "\n",
    "add1_pos['capitalised_words'] = add1_pos['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_pos_ll, capital_words_pos_ul))\n",
    "add1_neg['capitalised_words'] = add1_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_neg_ll, capital_words_neg_ul))\n",
    "\n",
    "add2_pos['capitalised_words'] = add2_pos['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_pos_ll, capital_words_pos_ul))\n",
    "add2_neg['capitalised_words'] = add2_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_neg_ll, capital_words_neg_ul))\n",
    "\n",
    "add3_pos['capitalised_words'] = add3_pos['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_pos_ll, capital_words_pos_ul))\n",
    "add3_neg['capitalised_words'] = add3_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_neg_ll, capital_words_neg_ul))\n",
    "\n",
    "add4_pos['capitalised_words'] = add4_pos['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_pos_ll, capital_words_pos_ul))\n",
    "add4_neg['capitalised_words'] = add4_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    capital_words_neg_ll, capital_words_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['capitalised_words'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1a1d4a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_mentions\n",
      "Yes    35048\n",
      "No      3751\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check number of user mentions using previously defined function\n",
    "add_pos['user_mentions'] = add_pos['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_pos_ll, user_mentions_pos_ul))\n",
    "add_neg['user_mentions'] = add_neg['text'].apply(lambda x: check_limits(count_capital_words(x), \n",
    "                                                                    user_mentions_neg_ll, user_mentions_neg_ul))\n",
    "\n",
    "add1_pos['user_mentions'] = add1_pos['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_pos_ll, user_mentions_pos_ul))\n",
    "add1_neg['user_mentions'] = add1_neg['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_neg_ll, user_mentions_neg_ul))\n",
    "\n",
    "add2_pos['user_mentions'] = add2_pos['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_pos_ll, user_mentions_pos_ul))\n",
    "add2_neg['user_mentions'] = add2_neg['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_neg_ll, user_mentions_neg_ul))\n",
    "\n",
    "add3_pos['user_mentions'] = add3_pos['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_pos_ll, user_mentions_pos_ul))\n",
    "add3_neg['user_mentions'] = add3_neg['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_neg_ll, user_mentions_neg_ul))\n",
    "\n",
    "add4_pos['user_mentions'] = add4_pos['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_pos_ll, user_mentions_pos_ul))\n",
    "add4_neg['user_mentions'] = add4_neg['text'].apply(lambda x: check_limits(count_user_mentions(x), \n",
    "                                                                    user_mentions_neg_ll, user_mentions_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['user_mentions'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a3072bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation\n",
      "Yes    37418\n",
      "No      1381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of potentially relevant punctuation using previously defined function\n",
    "add_pos['punctuation'] = add_pos['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_pos_ll, punctuation_pos_ul))\n",
    "add_neg['punctuation'] = add_neg['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_neg_ll, punctuation_neg_ul))\n",
    "\n",
    "add1_pos['punctuation'] = add1_pos['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_pos_ll, punctuation_pos_ul))\n",
    "add1_neg['punctuation'] = add1_neg['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_neg_ll, punctuation_neg_ul))\n",
    "\n",
    "add2_pos['punctuation'] = add2_pos['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_pos_ll, punctuation_pos_ul))\n",
    "add2_neg['punctuation'] = add2_neg['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_neg_ll, punctuation_neg_ul))\n",
    "\n",
    "add3_pos['punctuation'] = add3_pos['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_pos_ll, punctuation_pos_ul))\n",
    "add3_neg['punctuation'] = add3_neg['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_neg_ll, punctuation_neg_ul))\n",
    "\n",
    "add4_pos['punctuation'] = add4_pos['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_pos_ll, punctuation_pos_ul))\n",
    "add4_neg['punctuation'] = add4_neg['text'].apply(lambda x: check_limits(count_punctuation(x, punctuation), \n",
    "                                                                    punctuation_neg_ll, punctuation_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['punctuation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5309bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmatives\n",
      "Yes    38425\n",
      "No       374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of affirmatives using previously defined function\n",
    "add_pos['affirmatives'] = add_pos['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_pos_ll, affirmatives_pos_ul))\n",
    "add_neg['affirmatives'] = add_neg['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_neg_ll, affirmatives_neg_ul))\n",
    "\n",
    "add1_pos['affirmatives'] = add1_pos['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_pos_ll, affirmatives_pos_ul))\n",
    "add1_neg['affirmatives'] = add1_neg['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_neg_ll, affirmatives_neg_ul))\n",
    "\n",
    "add2_pos['affirmatives'] = add2_pos['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_pos_ll, affirmatives_pos_ul))\n",
    "add2_neg['affirmatives'] = add2_neg['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_neg_ll, affirmatives_neg_ul))\n",
    "\n",
    "add3_pos['affirmatives'] = add3_pos['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_pos_ll, affirmatives_pos_ul))\n",
    "add3_neg['affirmatives'] = add3_neg['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_neg_ll, affirmatives_neg_ul))\n",
    "\n",
    "add4_pos['affirmatives'] = add4_pos['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_pos_ll, affirmatives_pos_ul))\n",
    "add4_neg['affirmatives'] = add4_neg['text'].apply(lambda x: check_limits(count_affirmatives(x, strong_affirmatives), \n",
    "                                                                    affirmatives_neg_ll, affirmatives_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['affirmatives'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "907c214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negations\n",
      "Yes    36011\n",
      "No      2788\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of negations using previously defined function\n",
    "add_pos['negations'] = add_pos['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_pos_ll, negations_pos_ul))\n",
    "add_neg['negations'] = add_neg['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_neg_ll, negations_neg_ul))\n",
    "\n",
    "add1_pos['negations'] = add1_pos['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_pos_ll, negations_pos_ul))\n",
    "add1_neg['negations'] = add1_neg['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_neg_ll, negations_neg_ul))\n",
    "\n",
    "add2_pos['negations'] = add2_pos['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_pos_ll, negations_pos_ul))\n",
    "add2_neg['negations'] = add2_neg['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_neg_ll, negations_neg_ul))\n",
    "\n",
    "add3_pos['negations'] = add3_pos['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_pos_ll, negations_pos_ul))\n",
    "add3_neg['negations'] = add3_neg['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_neg_ll, negations_neg_ul))\n",
    "\n",
    "add4_pos['negations'] = add4_pos['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_pos_ll, negations_pos_ul))\n",
    "add4_neg['negations'] = add4_neg['text'].apply(lambda x: check_limits(count_negation(x, strong_negations), \n",
    "                                                                    negations_neg_ll, negations_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['negations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "360a475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensifiers\n",
      "Yes    38319\n",
      "No       480\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of intensifiers using previously defined function\n",
    "add_pos['intensifiers'] = add_pos['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_pos_ll, intensifiers_pos_ul))\n",
    "add_neg['intensifiers'] = add_neg['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_neg_ll, intensifiers_neg_ul))\n",
    "\n",
    "add1_pos['intensifiers'] = add1_pos['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_pos_ll, intensifiers_pos_ul))\n",
    "add1_neg['intensifiers'] = add1_neg['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_neg_ll, intensifiers_neg_ul))\n",
    "\n",
    "add2_pos['intensifiers'] = add2_pos['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_pos_ll, intensifiers_pos_ul))\n",
    "add2_neg['intensifiers'] = add2_neg['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_neg_ll, intensifiers_neg_ul))\n",
    "\n",
    "add3_pos['intensifiers'] = add3_pos['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_pos_ll, intensifiers_pos_ul))\n",
    "add3_neg['intensifiers'] = add3_neg['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_neg_ll, intensifiers_neg_ul))\n",
    "\n",
    "add4_pos['intensifiers'] = add4_pos['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_pos_ll, intensifiers_pos_ul))\n",
    "add4_neg['intensifiers'] = add4_neg['text'].apply(lambda x: check_limits(count_intensifiers(x, intensifiers), \n",
    "                                                                    intensifiers_neg_ll, intensifiers_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['intensifiers'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6ce6dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interjections\n",
      "Yes    38444\n",
      "No       355\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of interjections using previously defined function\n",
    "add_pos['interjections'] = add_pos['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_pos_ll, interjections_pos_ul))\n",
    "add_neg['interjections'] = add_neg['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_neg_ll, interjections_neg_ul))\n",
    "\n",
    "add1_pos['interjections'] = add1_pos['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_pos_ll, interjections_pos_ul))\n",
    "add1_neg['interjections'] = add1_neg['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_neg_ll, interjections_neg_ul))\n",
    "\n",
    "add2_pos['interjections'] = add2_pos['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_pos_ll, interjections_pos_ul))\n",
    "add2_neg['interjections'] = add2_neg['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_neg_ll, interjections_neg_ul))\n",
    "\n",
    "add3_pos['interjections'] = add3_pos['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_pos_ll, interjections_pos_ul))\n",
    "add3_neg['interjections'] = add3_neg['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_neg_ll, interjections_neg_ul))\n",
    "\n",
    "add4_pos['interjections'] = add4_pos['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_pos_ll, interjections_pos_ul))\n",
    "add4_neg['interjections'] = add4_neg['text'].apply(lambda x: check_limits(count_interjections(x, interjections), \n",
    "                                                                    interjections_neg_ll, interjections_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['interjections'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1ed2136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midword_capitalisations\n",
      "Yes    33549\n",
      "No      5250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check use of mid-word capitalisations using previously defined function\n",
    "add_pos['midword_capitalisations'] = add_pos['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_pos_ll, midword_caps_pos_ul))\n",
    "add_neg['midword_capitalisations'] = add_neg['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_neg_ll, midword_caps_neg_ul))\n",
    "\n",
    "add1_pos['midword_capitalisations'] = add1_pos['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_pos_ll, midword_caps_pos_ul))\n",
    "add1_neg['midword_capitalisations'] = add1_neg['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_neg_ll, midword_caps_neg_ul))\n",
    "\n",
    "add2_pos['midword_capitalisations'] = add2_pos['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_pos_ll, midword_caps_pos_ul))\n",
    "add2_neg['midword_capitalisations'] = add2_neg['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_neg_ll, midword_caps_neg_ul))\n",
    "\n",
    "add3_pos['midword_capitalisations'] = add3_pos['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_pos_ll, midword_caps_pos_ul))\n",
    "add3_neg['midword_capitalisations'] = add3_neg['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_neg_ll, midword_caps_neg_ul))\n",
    "\n",
    "add4_pos['midword_capitalisations'] = add4_pos['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_pos_ll, midword_caps_pos_ul))\n",
    "add4_neg['midword_capitalisations'] = add4_neg['text'].apply(lambda x: check_limits(count_midword_capitalisation(x), \n",
    "                                                                    midword_caps_neg_ll, midword_caps_neg_ul))\n",
    "\n",
    "#Check an outcome\n",
    "print(add_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a9309",
   "metadata": {},
   "source": [
    "# Filtering first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "04e98973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_result</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>emojis_per tweet</th>\n",
       "      <th>emojis_per_tweet</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>pred_neg</th>\n",
       "      <th>pred_neut</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>laughter</th>\n",
       "      <th>capitalised_words</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>affirmatives</th>\n",
       "      <th>negations</th>\n",
       "      <th>intensifiers</th>\n",
       "      <th>interjections</th>\n",
       "      <th>midword_capitalisations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tune in to Nigezie and be treated to Rachel Pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02273799665272236, 0.6872389316558838, 0.29...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@raaachf for the car ride when I get to listen...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.004537248983979225, 0.056863728910684586, 0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aamir calls #BajrangiBhaijaan as Salman's best...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.015803156420588493, 0.35204407572746277, 0....</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@stl7thward @LauraKHettiger I thought hot spot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.06396440416574478, 0.3994062542915344, 0.53...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I dont think any TV show could be more #Epic t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0027551085222512484, 0.015218231827020645, ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label   \n",
       "5   Tune in to Nigezie and be treated to Rachel Pl...      1  \\\n",
       "8   @raaachf for the car ride when I get to listen...      1   \n",
       "9   Aamir calls #BajrangiBhaijaan as Salman's best...      1   \n",
       "13  @stl7thward @LauraKHettiger I thought hot spot...      1   \n",
       "15  I dont think any TV show could be more #Epic t...      1   \n",
       "\n",
       "                                     sentiment_result sentiment_label   \n",
       "5   [0.02273799665272236, 0.6872389316558838, 0.29...        positive  \\\n",
       "8   [0.004537248983979225, 0.056863728910684586, 0...        positive   \n",
       "9   [0.015803156420588493, 0.35204407572746277, 0....        positive   \n",
       "13  [0.06396440416574478, 0.3994062542915344, 0.53...        positive   \n",
       "15  [0.0027551085222512484, 0.015218231827020645, ...        positive   \n",
       "\n",
       "   emojis_per tweet emojis_per_tweet sent_score pred_pos pred_neg pred_neut   \n",
       "5               Yes              Yes        NaN      NaN      NaN       NaN  \\\n",
       "8               Yes              Yes        NaN      NaN      NaN       NaN   \n",
       "9               Yes              Yes        NaN      NaN      NaN       NaN   \n",
       "13              Yes              Yes        NaN      NaN      NaN       NaN   \n",
       "15              Yes              Yes        NaN      NaN      NaN       NaN   \n",
       "\n",
       "   hashtags laughter capitalised_words user_mentions punctuation affirmatives   \n",
       "5        No      Yes                No           Yes         Yes          Yes  \\\n",
       "8       Yes      Yes               Yes           Yes         Yes          Yes   \n",
       "9       Yes      Yes               Yes           Yes         Yes          Yes   \n",
       "13      Yes      Yes               Yes            No         Yes          Yes   \n",
       "15       No      Yes               Yes           Yes          No          Yes   \n",
       "\n",
       "   negations intensifiers interjections midword_capitalisations  \n",
       "5        Yes          Yes           Yes                     Yes  \n",
       "8        Yes          Yes           Yes                     Yes  \n",
       "9        Yes          Yes           Yes                     Yes  \n",
       "13       Yes          Yes           Yes                     Yes  \n",
       "15       Yes          Yes           Yes                     Yes  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the positive df first\n",
    "add_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3f7464e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    38273\n",
      "No       526\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    3435\n",
      "No      226\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    3524\n",
      "No      137\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    3498\n",
      "No      163\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    3275\n",
      "No      386\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    21780\n",
      "No     17019\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    38096\n",
      "No       703\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    36758\n",
      "No      2041\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    35048\n",
      "No      3751\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    37418\n",
      "No      1381\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    38425\n",
      "No       374\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    36011\n",
      "No      2788\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    38319\n",
      "No       480\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    38444\n",
      "No       355\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    33549\n",
      "No      5250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check the stats for each metric\n",
    "print(add_pos['emojis_per_tweet'].value_counts())\n",
    "print(add_pos['sent_score'].value_counts())\n",
    "print(add_pos['pred_pos'].value_counts())\n",
    "print(add_pos['pred_neut'].value_counts())\n",
    "print(add_pos['pred_neg'].value_counts())\n",
    "print(add_pos['hashtags'].value_counts())\n",
    "print(add_pos['laughter'].value_counts())\n",
    "print(add_pos['capitalised_words'].value_counts())\n",
    "print(add_pos['user_mentions'].value_counts())\n",
    "print(add_pos['punctuation'].value_counts())\n",
    "print(add_pos['affirmatives'].value_counts())\n",
    "print(add_pos['negations'].value_counts())\n",
    "print(add_pos['intensifiers'].value_counts())\n",
    "print(add_pos['interjections'].value_counts())\n",
    "print(add_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a932a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "1    15132\n",
      "0    14775\n",
      "2     6780\n",
      "3     1730\n",
      "4      257\n",
      "5       90\n",
      "6       26\n",
      "7        8\n",
      "8        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Create a list of the columns containing the metric results \n",
    "columns_to_count = ['emojis_per_tweet', 'sent_score', 'pred_pos', 'pred_neut', 'pred_neg', 'hashtags', 'laughter',\n",
    "                   'capitalised_words', 'user_mentions', 'punctuation', 'affirmatives', 'negations', 'intensifiers',\n",
    "                   'interjections', 'midword_capitalisations']\n",
    "\n",
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add_pos[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add_pos['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add_pos['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "34ca208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 17019, 'capitalised_words': 2041, 'user_mentions': 3751, 'punctuation': 1381, 'negations': 2788, 'intensifiers': 480, 'emojis_per tweet': 526, 'emojis_per_tweet': 526, 'midword_capitalisations': 5250, 'laughter': 703, 'interjections': 355, 'affirmatives': 374, 'sent_score': 226, 'pred_neg': 386, 'pred_neut': 163, 'pred_pos': 137}\n"
     ]
    }
   ],
   "source": [
    "#Look at instances where atypical results appear\n",
    "\n",
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Define a function to find the column name with 'No' and its count for each row\n",
    "def find_column_with_no(row):\n",
    "    if row['no_count'] > 0:\n",
    "        for col_name in row.index:\n",
    "            if row[col_name] == 'No':\n",
    "                if col_name in columns_with_no:\n",
    "                    columns_with_no[col_name] += 1\n",
    "                else:\n",
    "                    columns_with_no[col_name] = 1\n",
    "\n",
    "#Apply the function\n",
    "add_pos.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "542806a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    50232\n",
      "No       503\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    2299\n",
      "No       85\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    2206\n",
      "No      178\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    2289\n",
      "No       95\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    2333\n",
      "No       51\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "No     35667\n",
      "Yes    15068\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    50067\n",
      "No       668\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    48080\n",
      "No      2655\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "No     30057\n",
      "Yes    20678\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    49885\n",
      "No       850\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    50161\n",
      "No       574\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    48801\n",
      "No      1934\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    50026\n",
      "No       709\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    50066\n",
      "No       669\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    44160\n",
      "No      6575\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Repeat for the negative subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add_neg['emojis_per_tweet'].value_counts())\n",
    "print(add_neg['sent_score'].value_counts())\n",
    "print(add_neg['pred_pos'].value_counts())\n",
    "print(add_neg['pred_neut'].value_counts())\n",
    "print(add_neg['pred_neg'].value_counts())\n",
    "print(add_neg['hashtags'].value_counts())\n",
    "print(add_neg['laughter'].value_counts())\n",
    "print(add_neg['capitalised_words'].value_counts())\n",
    "print(add_neg['user_mentions'].value_counts())\n",
    "print(add_neg['punctuation'].value_counts())\n",
    "print(add_neg['affirmatives'].value_counts())\n",
    "print(add_neg['negations'].value_counts())\n",
    "print(add_neg['intensifiers'].value_counts())\n",
    "print(add_neg['interjections'].value_counts())\n",
    "print(add_neg['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a40d5505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "1    18669\n",
      "2    18068\n",
      "3     6745\n",
      "0     5747\n",
      "4     1350\n",
      "5      118\n",
      "6       27\n",
      "7       10\n",
      "8        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add_neg[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add_neg['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add_neg['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "23a88bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 35667, 'user_mentions': 30057, 'punctuation': 850, 'midword_capitalisations': 6575, 'negations': 1934, 'capitalised_words': 2655, 'pred_pos': 178, 'affirmatives': 574, 'intensifiers': 709, 'interjections': 669, 'laughter': 668, 'emojis_per tweet': 503, 'emojis_per_tweet': 503, 'sent_score': 85, 'pred_neut': 95, 'pred_neg': 51}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add_neg.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942c0cc",
   "metadata": {},
   "source": [
    "# Filtering second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "335cf4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    6429\n",
      "No       67\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    1091\n",
      "No       80\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    1144\n",
      "No       27\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    1142\n",
      "No       29\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    999\n",
      "No     172\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    3963\n",
      "No     2533\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    6330\n",
      "No      166\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    6269\n",
      "No      227\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    6380\n",
      "No      116\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    6423\n",
      "No       73\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    6374\n",
      "No      122\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    4249\n",
      "No     2247\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    6370\n",
      "No      126\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    6393\n",
      "No      103\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    6334\n",
      "No      162\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Positive subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add1_pos['emojis_per_tweet'].value_counts())\n",
    "print(add1_pos['sent_score'].value_counts())\n",
    "print(add1_pos['pred_pos'].value_counts())\n",
    "print(add1_pos['pred_neut'].value_counts())\n",
    "print(add1_pos['pred_neg'].value_counts())\n",
    "print(add1_pos['hashtags'].value_counts())\n",
    "print(add1_pos['laughter'].value_counts())\n",
    "print(add1_pos['capitalised_words'].value_counts())\n",
    "print(add1_pos['user_mentions'].value_counts())\n",
    "print(add1_pos['punctuation'].value_counts())\n",
    "print(add1_pos['affirmatives'].value_counts())\n",
    "print(add1_pos['negations'].value_counts())\n",
    "print(add1_pos['intensifiers'].value_counts())\n",
    "print(add1_pos['interjections'].value_counts())\n",
    "print(add1_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3042744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    2736\n",
      "2    1739\n",
      "1    1697\n",
      "3     240\n",
      "4      71\n",
      "5       8\n",
      "6       4\n",
      "7       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add1_pos[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add1_pos['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add1_pos['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e88f254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negations': 2247, 'capitalised_words': 227, 'hashtags': 2533, 'midword_capitalisations': 162, 'laughter': 166, 'interjections': 103, 'pred_neg': 172, 'intensifiers': 126, 'user_mentions': 116, 'affirmatives': 122, 'sent_score': 80, 'emojis_per tweet': 67, 'emojis_per_tweet': 67, 'punctuation': 73, 'pred_pos': 27, 'pred_neut': 29}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add1_pos.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d7e43fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    12751\n",
      "No       241\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    1198\n",
      "No       72\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    1167\n",
      "No      103\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    1190\n",
      "No       80\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    1247\n",
      "No       23\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "No     12924\n",
      "Yes       68\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    12795\n",
      "No       197\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    12373\n",
      "No       619\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    12709\n",
      "No       283\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    12940\n",
      "No        52\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    12640\n",
      "No       352\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    11825\n",
      "No      1167\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    12660\n",
      "No       332\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    12616\n",
      "No       376\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    12624\n",
      "No       368\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Repeat for the negative subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add1_neg['emojis_per_tweet'].value_counts())\n",
    "print(add1_neg['sent_score'].value_counts())\n",
    "print(add1_neg['pred_pos'].value_counts())\n",
    "print(add1_neg['pred_neut'].value_counts())\n",
    "print(add1_neg['pred_neg'].value_counts())\n",
    "print(add1_neg['hashtags'].value_counts())\n",
    "print(add1_neg['laughter'].value_counts())\n",
    "print(add1_neg['capitalised_words'].value_counts())\n",
    "print(add1_neg['user_mentions'].value_counts())\n",
    "print(add1_neg['punctuation'].value_counts())\n",
    "print(add1_neg['affirmatives'].value_counts())\n",
    "print(add1_neg['negations'].value_counts())\n",
    "print(add1_neg['intensifiers'].value_counts())\n",
    "print(add1_neg['interjections'].value_counts())\n",
    "print(add1_neg['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "203cc7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "1    9657\n",
      "2    2516\n",
      "3     632\n",
      "4     104\n",
      "0      47\n",
      "5      28\n",
      "6       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add1_neg[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add1_neg['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add1_neg['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "bda67648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emojis_per tweet': 241, 'emojis_per_tweet': 241, 'hashtags': 12924, 'negations': 1167, 'intensifiers': 332, 'capitalised_words': 619, 'midword_capitalisations': 368, 'user_mentions': 283, 'laughter': 197, 'punctuation': 52, 'sent_score': 72, 'pred_pos': 103, 'pred_neut': 80, 'affirmatives': 352, 'interjections': 376, 'pred_neg': 23}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add1_neg.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bd95b",
   "metadata": {},
   "source": [
    "# Filtering third dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "29583c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    1296\n",
      "No        8\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    76\n",
      "No     20\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    80\n",
      "No     16\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    80\n",
      "No     16\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    56\n",
      "No     40\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    1232\n",
      "No       72\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    1252\n",
      "No       52\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    1240\n",
      "No       64\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    1232\n",
      "No       72\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    1236\n",
      "No       68\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    1300\n",
      "No        4\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    1208\n",
      "No       96\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    1304\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    1292\n",
      "No       12\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    1264\n",
      "No       40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Positive subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add2_pos['emojis_per_tweet'].value_counts())\n",
    "print(add2_pos['sent_score'].value_counts())\n",
    "print(add2_pos['pred_pos'].value_counts())\n",
    "print(add2_pos['pred_neut'].value_counts())\n",
    "print(add2_pos['pred_neg'].value_counts())\n",
    "print(add2_pos['hashtags'].value_counts())\n",
    "print(add2_pos['laughter'].value_counts())\n",
    "print(add2_pos['capitalised_words'].value_counts())\n",
    "print(add2_pos['user_mentions'].value_counts())\n",
    "print(add2_pos['punctuation'].value_counts())\n",
    "print(add2_pos['affirmatives'].value_counts())\n",
    "print(add2_pos['negations'].value_counts())\n",
    "print(add2_pos['intensifiers'].value_counts())\n",
    "print(add2_pos['interjections'].value_counts())\n",
    "print(add2_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9e9d5606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    876\n",
      "1    328\n",
      "2     72\n",
      "4     24\n",
      "3      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add2_pos[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add2_pos['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add2_pos['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1760fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitalised_words': 64, 'midword_capitalisations': 40, 'pred_neg': 40, 'user_mentions': 72, 'interjections': 12, 'hashtags': 72, 'punctuation': 68, 'laughter': 52, 'negations': 96, 'emojis_per tweet': 8, 'emojis_per_tweet': 8, 'affirmatives': 4, 'sent_score': 20, 'pred_pos': 16, 'pred_neut': 16}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add2_pos.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d448c6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    636\n",
      "No      16\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    24\n",
      "No      8\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    24\n",
      "No      8\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    24\n",
      "No      8\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    24\n",
      "No      8\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    480\n",
      "No     172\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    644\n",
      "No       8\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    632\n",
      "No      20\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    628\n",
      "No      24\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    640\n",
      "No      12\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    640\n",
      "No      12\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    628\n",
      "No      24\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    648\n",
      "No       4\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    636\n",
      "No      16\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    620\n",
      "No      32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Repeat for the negative subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add2_neg['emojis_per_tweet'].value_counts())\n",
    "print(add2_neg['sent_score'].value_counts())\n",
    "print(add2_neg['pred_pos'].value_counts())\n",
    "print(add2_neg['pred_neut'].value_counts())\n",
    "print(add2_neg['pred_neg'].value_counts())\n",
    "print(add2_neg['hashtags'].value_counts())\n",
    "print(add2_neg['laughter'].value_counts())\n",
    "print(add2_neg['capitalised_words'].value_counts())\n",
    "print(add2_neg['user_mentions'].value_counts())\n",
    "print(add2_neg['punctuation'].value_counts())\n",
    "print(add2_neg['affirmatives'].value_counts())\n",
    "print(add2_neg['negations'].value_counts())\n",
    "print(add2_neg['intensifiers'].value_counts())\n",
    "print(add2_neg['interjections'].value_counts())\n",
    "print(add2_neg['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f7577531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    380\n",
      "1    212\n",
      "2     40\n",
      "3     12\n",
      "7      4\n",
      "4      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add2_neg[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add2_neg['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add2_neg['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "66fdb4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 172, 'sent_score': 8, 'pred_pos': 8, 'pred_neg': 8, 'pred_neut': 8, 'user_mentions': 24, 'intensifiers': 4, 'midword_capitalisations': 32, 'laughter': 8, 'negations': 24, 'capitalised_words': 20, 'interjections': 16, 'emojis_per tweet': 16, 'emojis_per_tweet': 16, 'punctuation': 12, 'affirmatives': 12}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add2_neg.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd5b5c",
   "metadata": {},
   "source": [
    "# Filtering fourth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b87d8083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    11498\n",
      "No       115\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    1618\n",
      "No      117\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    1687\n",
      "No       48\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    1680\n",
      "No       55\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    1504\n",
      "No      231\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    10058\n",
      "No      1555\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    11329\n",
      "No       284\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    10995\n",
      "No       618\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    11126\n",
      "No       487\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    11410\n",
      "No       203\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    11386\n",
      "No       227\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    8633\n",
      "No     2980\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    11379\n",
      "No       234\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    11421\n",
      "No       192\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    11139\n",
      "No       474\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Positive subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add3_pos['emojis_per_tweet'].value_counts())\n",
    "print(add3_pos['sent_score'].value_counts())\n",
    "print(add3_pos['pred_pos'].value_counts())\n",
    "print(add3_pos['pred_neut'].value_counts())\n",
    "print(add3_pos['pred_neg'].value_counts())\n",
    "print(add3_pos['hashtags'].value_counts())\n",
    "print(add3_pos['laughter'].value_counts())\n",
    "print(add3_pos['capitalised_words'].value_counts())\n",
    "print(add3_pos['user_mentions'].value_counts())\n",
    "print(add3_pos['punctuation'].value_counts())\n",
    "print(add3_pos['affirmatives'].value_counts())\n",
    "print(add3_pos['negations'].value_counts())\n",
    "print(add3_pos['intensifiers'].value_counts())\n",
    "print(add3_pos['interjections'].value_counts())\n",
    "print(add3_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8889267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    6216\n",
      "1    3506\n",
      "2    1502\n",
      "3     299\n",
      "4      60\n",
      "5      14\n",
      "6      11\n",
      "7       4\n",
      "9       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add3_pos[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add3_pos['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add3_pos['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "24db18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negations': 2980, 'intensifiers': 234, 'capitalised_words': 618, 'punctuation': 203, 'hashtags': 1555, 'midword_capitalisations': 474, 'user_mentions': 487, 'sent_score': 117, 'pred_neg': 231, 'laughter': 284, 'affirmatives': 227, 'interjections': 192, 'emojis_per tweet': 115, 'emojis_per_tweet': 115, 'pred_pos': 48, 'pred_neut': 55}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add3_pos.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "52c81a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    13964\n",
      "No       284\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "Yes    1221\n",
      "No       42\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "Yes    1195\n",
      "No       68\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "Yes    1214\n",
      "No       49\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "Yes    1246\n",
      "No       17\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    10044\n",
      "No      4204\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    14025\n",
      "No       223\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    13512\n",
      "No       736\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    13531\n",
      "No       717\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    14134\n",
      "No       114\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    13893\n",
      "No       355\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    12872\n",
      "No      1376\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    13903\n",
      "No       345\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    13839\n",
      "No       409\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    13622\n",
      "No       626\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Repeat for the negative subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add3_neg['emojis_per_tweet'].value_counts())\n",
    "print(add3_neg['sent_score'].value_counts())\n",
    "print(add3_neg['pred_pos'].value_counts())\n",
    "print(add3_neg['pred_neut'].value_counts())\n",
    "print(add3_neg['pred_neg'].value_counts())\n",
    "print(add3_neg['hashtags'].value_counts())\n",
    "print(add3_neg['laughter'].value_counts())\n",
    "print(add3_neg['capitalised_words'].value_counts())\n",
    "print(add3_neg['user_mentions'].value_counts())\n",
    "print(add3_neg['punctuation'].value_counts())\n",
    "print(add3_neg['affirmatives'].value_counts())\n",
    "print(add3_neg['negations'].value_counts())\n",
    "print(add3_neg['intensifiers'].value_counts())\n",
    "print(add3_neg['interjections'].value_counts())\n",
    "print(add3_neg['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a61fd214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    7480\n",
      "1    4536\n",
      "2    1754\n",
      "3     405\n",
      "4      64\n",
      "5       5\n",
      "6       3\n",
      "7       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add3_neg[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add3_neg['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add3_neg['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3b9aa530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 4204, 'user_mentions': 717, 'negations': 1376, 'emojis_per tweet': 284, 'emojis_per_tweet': 284, 'midword_capitalisations': 626, 'laughter': 223, 'pred_pos': 68, 'interjections': 409, 'intensifiers': 345, 'capitalised_words': 736, 'affirmatives': 355, 'punctuation': 114, 'sent_score': 42, 'pred_neg': 17, 'pred_neut': 49}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add3_neg.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3ed28",
   "metadata": {},
   "source": [
    "# Filtering fifth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "19cf6470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    608\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "No    13\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "No    13\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "No    13\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "No    13\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "Yes    482\n",
      "No     126\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    601\n",
      "No       7\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    601\n",
      "No       7\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    608\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    587\n",
      "No      21\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    602\n",
      "No       6\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    338\n",
      "No     270\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    598\n",
      "No      10\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    604\n",
      "No       4\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    599\n",
      "No       9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Positive subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add4_pos['emojis_per_tweet'].value_counts())\n",
    "print(add4_pos['sent_score'].value_counts())\n",
    "print(add4_pos['pred_pos'].value_counts())\n",
    "print(add4_pos['pred_neut'].value_counts())\n",
    "print(add4_pos['pred_neg'].value_counts())\n",
    "print(add4_pos['hashtags'].value_counts())\n",
    "print(add4_pos['laughter'].value_counts())\n",
    "print(add4_pos['capitalised_words'].value_counts())\n",
    "print(add4_pos['user_mentions'].value_counts())\n",
    "print(add4_pos['punctuation'].value_counts())\n",
    "print(add4_pos['affirmatives'].value_counts())\n",
    "print(add4_pos['negations'].value_counts())\n",
    "print(add4_pos['intensifiers'].value_counts())\n",
    "print(add4_pos['interjections'].value_counts())\n",
    "print(add4_pos['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e54360e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "0    251\n",
      "1    251\n",
      "2     83\n",
      "3      9\n",
      "4      6\n",
      "6      4\n",
      "5      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add4_pos[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add4_pos['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add4_pos['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5fd0238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 126, 'punctuation': 21, 'negations': 270, 'midword_capitalisations': 9, 'laughter': 7, 'capitalised_words': 7, 'affirmatives': 6, 'interjections': 4, 'sent_score': 13, 'pred_pos': 13, 'pred_neg': 13, 'pred_neut': 13, 'intensifiers': 10}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add4_pos.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "697ae8b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojis_per_tweet\n",
      "Yes    350\n",
      "Name: count, dtype: int64\n",
      "sent_score\n",
      "No    7\n",
      "Name: count, dtype: int64\n",
      "pred_pos\n",
      "No    7\n",
      "Name: count, dtype: int64\n",
      "pred_neut\n",
      "No    7\n",
      "Name: count, dtype: int64\n",
      "pred_neg\n",
      "No    7\n",
      "Name: count, dtype: int64\n",
      "hashtags\n",
      "No     234\n",
      "Yes    116\n",
      "Name: count, dtype: int64\n",
      "laughter\n",
      "Yes    345\n",
      "No       5\n",
      "Name: count, dtype: int64\n",
      "capitalised_words\n",
      "Yes    343\n",
      "No       7\n",
      "Name: count, dtype: int64\n",
      "user_mentions\n",
      "Yes    350\n",
      "Name: count, dtype: int64\n",
      "punctuation\n",
      "Yes    347\n",
      "No       3\n",
      "Name: count, dtype: int64\n",
      "affirmatives\n",
      "Yes    341\n",
      "No       9\n",
      "Name: count, dtype: int64\n",
      "negations\n",
      "Yes    275\n",
      "No      75\n",
      "Name: count, dtype: int64\n",
      "intensifiers\n",
      "Yes    334\n",
      "No      16\n",
      "Name: count, dtype: int64\n",
      "interjections\n",
      "Yes    348\n",
      "No       2\n",
      "Name: count, dtype: int64\n",
      "midword_capitalisations\n",
      "Yes    341\n",
      "No       9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Repeat for the negative subset\n",
    "\n",
    "#Check the stats for each metric\n",
    "print(add4_neg['emojis_per_tweet'].value_counts())\n",
    "print(add4_neg['sent_score'].value_counts())\n",
    "print(add4_neg['pred_pos'].value_counts())\n",
    "print(add4_neg['pred_neut'].value_counts())\n",
    "print(add4_neg['pred_neg'].value_counts())\n",
    "print(add4_neg['hashtags'].value_counts())\n",
    "print(add4_neg['laughter'].value_counts())\n",
    "print(add4_neg['capitalised_words'].value_counts())\n",
    "print(add4_neg['user_mentions'].value_counts())\n",
    "print(add4_neg['punctuation'].value_counts())\n",
    "print(add4_neg['affirmatives'].value_counts())\n",
    "print(add4_neg['negations'].value_counts())\n",
    "print(add4_neg['intensifiers'].value_counts())\n",
    "print(add4_neg['interjections'].value_counts())\n",
    "print(add4_neg['midword_capitalisations'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "faf17629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_count\n",
      "1    174\n",
      "0     85\n",
      "2     74\n",
      "3      9\n",
      "5      4\n",
      "4      3\n",
      "7      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Use boolean indexing to create a df to count the number of times atypical results were obtained for each tweet\n",
    "no_df = (add4_neg[columns_to_count] == 'No')\n",
    "\n",
    "#Store sum of true results in the df\n",
    "add4_neg['no_count'] = no_df.sum(axis=1)\n",
    "\n",
    "#Print results\n",
    "print(add4_neg['no_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "73d25f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': 234, 'negations': 75, 'affirmatives': 9, 'intensifiers': 16, 'capitalised_words': 7, 'sent_score': 7, 'pred_pos': 7, 'pred_neg': 7, 'pred_neut': 7, 'punctuation': 3, 'laughter': 5, 'midword_capitalisations': 9, 'interjections': 2}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary to store column names where the atypical result was obtained \n",
    "columns_with_no = {}\n",
    "\n",
    "#Apply the function to determine metrics which cause atypical results and their frequencies\n",
    "add4_neg.apply(find_column_with_no, axis=1)\n",
    "\n",
    "#Print results\n",
    "print(columns_with_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04eb736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5044e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c5478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f7d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f36c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b8bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
